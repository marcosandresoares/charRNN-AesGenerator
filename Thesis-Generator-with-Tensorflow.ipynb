{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 documents\n"
     ]
    }
   ],
   "source": [
    "thesis_filenames = sorted(glob.glob(\"data_album/theses_v1.txt\"))\n",
    "\n",
    "print(\"Found {} documents\".format(len(thesis_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 532721 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for filename in thesis_filenames:\n",
    "    with codecs.open(filename, 'r', 'utf-8') as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_album/theses_v1.txt', 'r') as text_analysis:\n",
    "    txt = text_analysis.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_analysis = set(txt.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 3,\n",
       " 'Sharing': 1,\n",
       " 'products’': 2,\n",
       " 'reiterated': 1,\n",
       " 'Keller': 1,\n",
       " '(Dobbs': 2,\n",
       " 'generation': 3,\n",
       " 'pilot': 1,\n",
       " 'apply': 10,\n",
       " 'photo': 2,\n",
       " '(1931),': 1,\n",
       " 'legal': 2,\n",
       " 'linearly': 1,\n",
       " '(P1,': 2,\n",
       " 'users': 116,\n",
       " 'probability.': 1,\n",
       " 'plastic': 3,\n",
       " 'returned': 2,\n",
       " 'keyboard.': 1,\n",
       " 'Lifestyle,': 1,\n",
       " 'philosophical': 1,\n",
       " 'cardboard': 2,\n",
       " 'them’.': 1,\n",
       " 'elsewhere.': 1,\n",
       " 'students': 2,\n",
       " 'Searching': 1,\n",
       " '2,': 2,\n",
       " 'Dorst': 1,\n",
       " 'Capitalism\\nNatural': 1,\n",
       " 'Textiles\\nThe': 1,\n",
       " 'tool,': 3,\n",
       " '\\nAlternative': 1,\n",
       " 'fabric.': 3,\n",
       " 'construct': 4,\n",
       " 'mentor': 1,\n",
       " 'periods.': 2,\n",
       " '2)': 2,\n",
       " 'collections:': 3,\n",
       " 'processing': 5,\n",
       " 'metadata”': 1,\n",
       " 'record.': 1,\n",
       " 'representative': 8,\n",
       " 'exploring': 17,\n",
       " 'post-its,': 1,\n",
       " 'event.': 2,\n",
       " 'related': 27,\n",
       " 'expose': 2,\n",
       " 'BETWEEN': 1,\n",
       " 'Hekkert,': 5,\n",
       " 'credit,': 1,\n",
       " 'illustrations.': 1,\n",
       " 'Orr’s': 1,\n",
       " 'prototype,': 2,\n",
       " 'Meeks,': 1,\n",
       " 'one’': 1,\n",
       " 'advancement,': 1,\n",
       " 'approach,': 10,\n",
       " 'pride': 2,\n",
       " 'fascinating.': 1,\n",
       " 'distinction': 4,\n",
       " 'McLaren': 2,\n",
       " 'Figure': 43,\n",
       " 'Year': 1,\n",
       " 'task.': 3,\n",
       " 'identifications,': 1,\n",
       " 'booklets': 1,\n",
       " 'collectivities': 1,\n",
       " 'Patches\\nInitially,': 1,\n",
       " 'limits': 4,\n",
       " 'Netherlands,': 1,\n",
       " 'nonuniform': 1,\n",
       " 'reviewing': 2,\n",
       " 'collaborate': 3,\n",
       " '\\nLack': 1,\n",
       " 'target': 16,\n",
       " 'guide': 5,\n",
       " 'space.': 3,\n",
       " 'shape,': 1,\n",
       " 'receive': 2,\n",
       " 'caution:': 1,\n",
       " 'resin': 1,\n",
       " '19).': 1,\n",
       " '(Gelman': 1,\n",
       " '(C7).': 1,\n",
       " 'genres': 1,\n",
       " 'archives,': 1,\n",
       " 'curated': 4,\n",
       " 'For': 57,\n",
       " 'does': 41,\n",
       " 'Lenglet': 1,\n",
       " 'accordance': 2,\n",
       " 'concepts.': 1,\n",
       " 'tries': 4,\n",
       " 'Erik': 1,\n",
       " 'p.135)': 1,\n",
       " 'summarises': 1,\n",
       " 'manageable': 2,\n",
       " '“complete”': 1,\n",
       " 'whatever.': 1,\n",
       " 'chair': 4,\n",
       " 'diagram’s': 1,\n",
       " 'electronic': 10,\n",
       " '(Salvia': 1,\n",
       " 'Cataloguing': 2,\n",
       " '\\n\\nEmbedding': 1,\n",
       " 'remanufacturing': 8,\n",
       " 'gold,': 2,\n",
       " 'online.': 2,\n",
       " 'time-aware': 2,\n",
       " 'affecting': 2,\n",
       " 'order,': 2,\n",
       " 'imitate': 1,\n",
       " 'curators,': 2,\n",
       " 'enforces': 1,\n",
       " 'realise.”': 1,\n",
       " 'Nevertheless,': 4,\n",
       " 'individuals,': 5,\n",
       " 'tolerate': 1,\n",
       " 'stating': 1,\n",
       " 'effectively.\\nInvolving': 1,\n",
       " 'piece': 15,\n",
       " 'practice.\\nThe': 2,\n",
       " 'decides': 1,\n",
       " 'peer': 3,\n",
       " 'impacts.': 2,\n",
       " 'tag': 1,\n",
       " 'backpack': 6,\n",
       " '“once': 1,\n",
       " 'counterpart': 1,\n",
       " 'specifically.': 1,\n",
       " 'Quantitative': 6,\n",
       " '\\nCatalogues': 1,\n",
       " 'compactness': 1,\n",
       " 'rubber': 1,\n",
       " '(Gomez': 1,\n",
       " 'reinterpretation': 1,\n",
       " 'All': 3,\n",
       " '(Lansdale': 1,\n",
       " 'index': 5,\n",
       " 'ambition': 1,\n",
       " 'discourses': 1,\n",
       " 'datasets,': 9,\n",
       " 'enjoyed': 10,\n",
       " 'broad,': 1,\n",
       " '\\nDesire': 1,\n",
       " 'archive': 17,\n",
       " 'said': 26,\n",
       " 'packaging': 1,\n",
       " 'maintaining': 3,\n",
       " 'explicating': 1,\n",
       " 'painted': 2,\n",
       " 'Three': 2,\n",
       " 'Incentivised': 1,\n",
       " 'Wakkary’s': 1,\n",
       " 'rooted': 1,\n",
       " 'released.': 1,\n",
       " 'useful.': 2,\n",
       " 'two-': 1,\n",
       " 'meaningful': 6,\n",
       " 'Stryker': 1,\n",
       " 'once.': 1,\n",
       " 'corners': 1,\n",
       " 'Kant,': 1,\n",
       " 'skin,': 1,\n",
       " 'inspired': 8,\n",
       " 'These': 54,\n",
       " 'rate': 1,\n",
       " 'DuPont': 1,\n",
       " 'damaging': 1,\n",
       " '(1750b),': 1,\n",
       " 'Ripped': 1,\n",
       " 'inevitable,': 1,\n",
       " 'throw': 6,\n",
       " '‘cradle': 1,\n",
       " 'important': 36,\n",
       " 'separately': 4,\n",
       " 'low-quality': 4,\n",
       " 'analysts': 1,\n",
       " 'rule-based': 1,\n",
       " 'ergonomic': 1,\n",
       " 'loaned': 1,\n",
       " 'careful,': 1,\n",
       " 'disruption': 1,\n",
       " 'specify': 2,\n",
       " 'shading.': 1,\n",
       " 'Tractinsky,': 1,\n",
       " 'Diagrams,': 1,\n",
       " 'ordered': 4,\n",
       " 'empire': 4,\n",
       " 'star-shaped': 1,\n",
       " 'total;': 1,\n",
       " '(largely)': 1,\n",
       " 'hypothesis': 1,\n",
       " 'import,': 1,\n",
       " 'workplace': 2,\n",
       " 'truer': 2,\n",
       " 'Spark’s': 1,\n",
       " 'one.': 9,\n",
       " 'archive,': 4,\n",
       " 'Small': 1,\n",
       " 'newer': 2,\n",
       " 'Industrial': 2,\n",
       " '4.19).': 1,\n",
       " 'Greater': 1,\n",
       " 'nine': 2,\n",
       " 'Ramakers': 1,\n",
       " 'slightly:': 1,\n",
       " 'originals;': 1,\n",
       " 'There’s': 1,\n",
       " 'addition,': 10,\n",
       " 'repair.\\nAfter': 1,\n",
       " 'follows:': 2,\n",
       " 'power.': 1,\n",
       " 'biases,': 2,\n",
       " 'lexicographical': 1,\n",
       " 'lacquer.': 1,\n",
       " 'biases.': 1,\n",
       " 'Climate': 1,\n",
       " '“probably”,': 1,\n",
       " 'combat': 1,\n",
       " 'Open': 1,\n",
       " 'online,': 1,\n",
       " '1,': 2,\n",
       " 'thoughtful': 2,\n",
       " 'these,': 1,\n",
       " 'no-one': 1,\n",
       " 'cognition.\\n\\nCollections\\nWhen': 1,\n",
       " 'Meeks': 2,\n",
       " 'cross-country': 1,\n",
       " 'fuels': 1,\n",
       " '‘to': 1,\n",
       " 'conclusion': 4,\n",
       " 'linkage.': 1,\n",
       " 'Metaball': 2,\n",
       " 'premature': 1,\n",
       " 'attach': 1,\n",
       " 'trajectory': 1,\n",
       " 'mug': 1,\n",
       " '“for': 1,\n",
       " 'potentially': 7,\n",
       " 'acquisition,': 1,\n",
       " 'Websites': 1,\n",
       " 'total.': 1,\n",
       " 'dependencies': 1,\n",
       " 'cameras': 1,\n",
       " 'p.38).': 1,\n",
       " 'importance.': 1,\n",
       " 'these': 166,\n",
       " 'attributed': 1,\n",
       " 'NYT': 3,\n",
       " 'shopping': 3,\n",
       " 'implied': 5,\n",
       " 'offered': 16,\n",
       " 'ceiling': 1,\n",
       " 'barrier': 9,\n",
       " 'choice,': 2,\n",
       " 'different': 152,\n",
       " 'decision': 13,\n",
       " 'inherently': 7,\n",
       " 'Swan,': 1,\n",
       " 'expanded': 1,\n",
       " 'Analytic': 1,\n",
       " 'producers;': 1,\n",
       " 'problems;': 1,\n",
       " 'tiny': 3,\n",
       " 'relies': 2,\n",
       " 'girlfriend’s': 1,\n",
       " 'categories,': 1,\n",
       " '“visitors': 1,\n",
       " '(Beltramin': 3,\n",
       " 'North': 2,\n",
       " '4.3.3).': 2,\n",
       " 'somewhat': 1,\n",
       " 'Chronographique': 1,\n",
       " 'helpful': 9,\n",
       " 'expectation': 4,\n",
       " 'success': 4,\n",
       " 'thickness,': 1,\n",
       " 'car,': 2,\n",
       " 'reduction).': 1,\n",
       " 'Edward': 1,\n",
       " 'employing': 3,\n",
       " 'modules': 1,\n",
       " 'coherence': 1,\n",
       " 'products:': 5,\n",
       " 'self-production.': 1,\n",
       " 'uses,': 1,\n",
       " 'adaptable,': 1,\n",
       " '(2014)\\nIn': 1,\n",
       " '“timeline': 1,\n",
       " 'forms': 13,\n",
       " 'oil-based': 2,\n",
       " 'cutting': 1,\n",
       " 'what,': 1,\n",
       " 'detachedly': 1,\n",
       " 'foundation': 5,\n",
       " 'perhaps,': 4,\n",
       " 'attain': 1,\n",
       " 'whereas': 5,\n",
       " 'exploratory': 14,\n",
       " 'Nielsen': 4,\n",
       " 'Slightly': 1,\n",
       " 'incorporated': 4,\n",
       " 'control': 7,\n",
       " 'certainly': 1,\n",
       " 'killed': 1,\n",
       " 'thematically': 1,\n",
       " 'invites': 3,\n",
       " 'ever-decreasing': 1,\n",
       " 'unit': 5,\n",
       " '1998;': 5,\n",
       " '\\nTechnological': 1,\n",
       " 'theorised': 2,\n",
       " 'inappropriate': 1,\n",
       " 'affinity': 3,\n",
       " 'prefer': 4,\n",
       " '93)': 1,\n",
       " '‘basin’.': 1,\n",
       " 'ideas.': 3,\n",
       " 'comparatively': 1,\n",
       " 'coined': 2,\n",
       " 'printed': 9,\n",
       " 'answered.': 2,\n",
       " 'selectors.': 1,\n",
       " 'orientation': 1,\n",
       " 'targeting': 2,\n",
       " 'knees.': 1,\n",
       " 'phone,': 1,\n",
       " 'prepared': 2,\n",
       " '‘using': 1,\n",
       " 'misused.\\n\\nFocus': 1,\n",
       " 'multidisciplinary': 1,\n",
       " 'territory,': 1,\n",
       " 'Repairers': 1,\n",
       " 'zero,': 1,\n",
       " 'differ.': 1,\n",
       " 'considered': 27,\n",
       " 'stores.': 1,\n",
       " '‘At': 1,\n",
       " 'salt,': 1,\n",
       " 'mending': 12,\n",
       " 'twenty-first': 1,\n",
       " 'respects': 2,\n",
       " 'inclusion': 1,\n",
       " 'before,': 2,\n",
       " 'Thorpe': 1,\n",
       " 'examination': 1,\n",
       " 'transparent': 12,\n",
       " '2004).': 7,\n",
       " 'candidates': 1,\n",
       " 'laddered': 1,\n",
       " 'focuses': 13,\n",
       " '1999/2006).\\n\\nIntrinsic': 1,\n",
       " 'establishment': 1,\n",
       " 'circular,': 2,\n",
       " 'aesthetic': 15,\n",
       " 'protests”,': 1,\n",
       " 'Various': 1,\n",
       " 'neutrality': 1,\n",
       " 'Solely': 1,\n",
       " 'facing': 1,\n",
       " 'may': 115,\n",
       " 'drawbacks.': 1,\n",
       " 'Study': 2,\n",
       " 'Hockey': 1,\n",
       " 'predict': 1,\n",
       " 'algorithm,': 1,\n",
       " '“circa”,': 1,\n",
       " 'Textual': 1,\n",
       " 'end': 27,\n",
       " 'never,': 1,\n",
       " 'Gain': 1,\n",
       " 'reminder,': 1,\n",
       " 'requirements': 11,\n",
       " 'documenting': 2,\n",
       " 'pretty': 1,\n",
       " 'Principles': 1,\n",
       " 'readings': 2,\n",
       " '‘natural’,': 1,\n",
       " 'buying': 8,\n",
       " 'Philadelphia,': 1,\n",
       " 'objectives': 3,\n",
       " 'Patches\\nThe': 1,\n",
       " 'certainty': 2,\n",
       " 'biological': 12,\n",
       " 'Moere': 1,\n",
       " 'comparisons': 6,\n",
       " 'essence': 3,\n",
       " 'events': 92,\n",
       " 'reports,': 1,\n",
       " 'actually,': 1,\n",
       " 'Dutch': 2,\n",
       " 'results': 35,\n",
       " '(C1)\\nCollections': 1,\n",
       " '‘invisible': 1,\n",
       " 'fit': 2,\n",
       " '(Braungart,': 1,\n",
       " 'Several': 1,\n",
       " 'hypothesis,': 1,\n",
       " '“into': 1,\n",
       " '1946-1989.': 1,\n",
       " 'workshops.\\n14.': 1,\n",
       " '(Chenhall': 1,\n",
       " 'develop': 24,\n",
       " 'electronics,': 1,\n",
       " 'Questions\\n\\n-': 1,\n",
       " 'fabrics,': 1,\n",
       " 'material.\\n3D': 1,\n",
       " 'classifying': 2,\n",
       " 'conducted': 18,\n",
       " 'included': 39,\n",
       " 'Moretti’s': 1,\n",
       " 'correct': 4,\n",
       " 'Kits?': 1,\n",
       " '“ca.”,': 1,\n",
       " 'Humanities': 24,\n",
       " 'circular': 86,\n",
       " 'Traditional': 2,\n",
       " 'subjects': 5,\n",
       " 'high,': 1,\n",
       " 'aspects.': 4,\n",
       " 'oxygen': 1,\n",
       " 'kintsugi': 23,\n",
       " 'conjunction': 1,\n",
       " 'Humanities.': 1,\n",
       " 'recognisable': 1,\n",
       " 'once': 7,\n",
       " '\\nFishing': 1,\n",
       " 'lid.': 1,\n",
       " 'Arnheim,': 1,\n",
       " 'forced': 4,\n",
       " 'assembling': 1,\n",
       " '1.': 1,\n",
       " 'dimensions': 6,\n",
       " 'benefits': 12,\n",
       " 'entirely': 1,\n",
       " 'beneficial.': 1,\n",
       " 'excavating': 1,\n",
       " '\\nWorkshops': 1,\n",
       " 'Central': 1,\n",
       " 'Xerox': 1,\n",
       " 'underlining': 1,\n",
       " 'manipulate,': 1,\n",
       " 'address.\\n\\nChapter': 1,\n",
       " 'freedom': 4,\n",
       " 'prevent': 3,\n",
       " 'reflects': 2,\n",
       " 'projects.': 7,\n",
       " 'children,': 1,\n",
       " 'diagnosed': 1,\n",
       " 'generous': 2,\n",
       " 'alignment': 1,\n",
       " 'undesirable': 2,\n",
       " 'convenience': 2,\n",
       " 'second,': 2,\n",
       " 'images': 3,\n",
       " 'Priestley': 11,\n",
       " 'repaired': 73,\n",
       " 'aggregating': 1,\n",
       " 'nutrients': 7,\n",
       " 'Heidegger,': 1,\n",
       " 'institutions,': 3,\n",
       " 'consumers’,': 1,\n",
       " 'parallel': 4,\n",
       " 'reveals': 7,\n",
       " 'dimensions,': 1,\n",
       " '‘misused’': 1,\n",
       " 'Labels': 1,\n",
       " 'root': 1,\n",
       " 'specifying': 3,\n",
       " 'rule': 3,\n",
       " 'advertising,': 1,\n",
       " 'mathematic': 1,\n",
       " 'confusion': 1,\n",
       " 'on': 565,\n",
       " '‘truths’:': 1,\n",
       " 'definitive': 1,\n",
       " 'superior': 2,\n",
       " 'historian,': 1,\n",
       " 'thing': 7,\n",
       " '26': 1,\n",
       " 'suits': 1,\n",
       " 'assumptions,': 3,\n",
       " '91).': 1,\n",
       " 'Google': 1,\n",
       " 'levels,': 1,\n",
       " 'section.\\n\\nCradle': 1,\n",
       " '(1967/2010,': 1,\n",
       " 'embedded': 24,\n",
       " 'whole': 10,\n",
       " 'comments,': 1,\n",
       " '(ibid.)': 3,\n",
       " 'back.': 1,\n",
       " 'London': 4,\n",
       " 'Taking': 3,\n",
       " 'actively': 1,\n",
       " 'consultation': 1,\n",
       " 'operations.': 1,\n",
       " '2.17,': 1,\n",
       " 'undergone': 1,\n",
       " 'improve,': 1,\n",
       " '(1733–1804),': 1,\n",
       " 'visualisations.\\n\\nUsers,': 1,\n",
       " 'Heinemeyer,': 2,\n",
       " 'Donella': 1,\n",
       " 'artist:': 1,\n",
       " 'implementations,': 4,\n",
       " 'Hassenzahl': 1,\n",
       " 'specific.': 1,\n",
       " 'Architects,': 2,\n",
       " '\\nTime-wise': 1,\n",
       " 'Testing\\nI': 1,\n",
       " 'downloaded': 1,\n",
       " 'materialistic': 1,\n",
       " '‘I': 12,\n",
       " 'shoe': 1,\n",
       " 'College': 4,\n",
       " 'willing': 5,\n",
       " 'slowly.': 1,\n",
       " 'Rau,': 1,\n",
       " 'lose': 3,\n",
       " 'mode': 3,\n",
       " '2.11': 1,\n",
       " 'histories': 5,\n",
       " 'comprised': 1,\n",
       " 'Pyramids': 1,\n",
       " 'literature': 31,\n",
       " 'accessed': 2,\n",
       " '2008;': 2,\n",
       " '2004;': 7,\n",
       " 'banks': 1,\n",
       " 'Conversations': 1,\n",
       " 'thus,': 2,\n",
       " 'p.2).\\nAlthough': 1,\n",
       " '(Hockey': 1,\n",
       " '“piano”': 1,\n",
       " 'concerned,': 2,\n",
       " 'spot': 6,\n",
       " '1753),': 1,\n",
       " 'noticed': 1,\n",
       " 'Research': 7,\n",
       " 'already': 28,\n",
       " 'booking': 1,\n",
       " 'forcing': 1,\n",
       " 'marking': 1,\n",
       " '3.1).': 1,\n",
       " 'inside': 8,\n",
       " 'devices,': 5,\n",
       " 'creating': 24,\n",
       " 'extended': 2,\n",
       " 'unique': 14,\n",
       " 'discoveries,': 1,\n",
       " 'List': 1,\n",
       " 'Dunne': 1,\n",
       " 'businesses.': 2,\n",
       " '“uninterpreted': 1,\n",
       " 'Knowledge,': 1,\n",
       " 'Tate\\n\\nProcess\\nAn': 1,\n",
       " 'proved': 6,\n",
       " 'hybrid': 3,\n",
       " 'visible': 26,\n",
       " '41).': 1,\n",
       " 'not.': 2,\n",
       " 'tension,': 1,\n",
       " 'Pennington,': 1,\n",
       " 'repairing,': 2,\n",
       " 'entity.': 1,\n",
       " 'having': 10,\n",
       " 'gold-coloured': 3,\n",
       " 'Davis.': 1,\n",
       " 'uses;': 1,\n",
       " 'it;': 2,\n",
       " 'B.': 1,\n",
       " 'sake': 2,\n",
       " 'someone’s': 1,\n",
       " 'collapse': 1,\n",
       " 'scholarship.': 4,\n",
       " '\\n2.': 2,\n",
       " 'reliable': 6,\n",
       " 'visualised': 18,\n",
       " 'engendered': 1,\n",
       " 'on;': 1,\n",
       " 'exactly': 7,\n",
       " 'spectrum': 9,\n",
       " '\\nAfter': 3,\n",
       " 'restricted': 2,\n",
       " 'Child': 1,\n",
       " 'Single': 1,\n",
       " 'offices.': 1,\n",
       " 'Eurozone': 1,\n",
       " '14%': 1,\n",
       " 'conversely': 1,\n",
       " 's/he': 8,\n",
       " 'assessment': 5,\n",
       " 'intersection': 4,\n",
       " 'Constitute': 1,\n",
       " 'fast-moving': 1,\n",
       " 'up,': 2,\n",
       " 'further.\\n\\nExploratory': 1,\n",
       " 'spaces.': 2,\n",
       " 'supplied': 1,\n",
       " 'waste’': 1,\n",
       " 'occupy.': 1,\n",
       " 'back': 21,\n",
       " 'la': 8,\n",
       " 'line': 21,\n",
       " 'evaluation': 17,\n",
       " 'rack': 1,\n",
       " 'seek': 3,\n",
       " 'small': 32,\n",
       " 'accomplish': 1,\n",
       " 'condensed': 2,\n",
       " 'Council': 1,\n",
       " 'Noesis': 1,\n",
       " 'today’s': 2,\n",
       " 'belonged': 1,\n",
       " 'so;': 1,\n",
       " 'took': 15,\n",
       " 'empathic': 1,\n",
       " 'implementation.': 1,\n",
       " 'visualisation?': 1,\n",
       " 'actually': 6,\n",
       " 'materialisation': 1,\n",
       " 'Zerubavel': 1,\n",
       " 'side-effects': 2,\n",
       " 'pair,': 1,\n",
       " 'miners': 1,\n",
       " 'advantage': 9,\n",
       " '“possibly': 1,\n",
       " 'plasters': 2,\n",
       " 'long': 23,\n",
       " 'consume': 5,\n",
       " 'revival,': 1,\n",
       " 'document': 3,\n",
       " 'interactive,': 1,\n",
       " 'play': 3,\n",
       " 'multiplicity,': 1,\n",
       " 'thread': 5,\n",
       " 'built': 7,\n",
       " 'thirty-five': 1,\n",
       " 'homes': 1,\n",
       " 'repeat': 2,\n",
       " 'conceptual': 10,\n",
       " 'viewer': 6,\n",
       " 'vicious': 1,\n",
       " 'Visualisation,': 1,\n",
       " 'assigned': 4,\n",
       " 'urge': 2,\n",
       " 'Storytelling': 1,\n",
       " 'closer': 8,\n",
       " 'Director': 1,\n",
       " 'error,': 1,\n",
       " 'compatible': 2,\n",
       " 'afford?\\n\\nPrototype': 1,\n",
       " '2.16.2,': 1,\n",
       " 'cancer': 1,\n",
       " 'unattractive': 1,\n",
       " '\\nFinal': 1,\n",
       " 'interested;': 1,\n",
       " 'lowers': 1,\n",
       " 'thesaurus': 1,\n",
       " 'spoke': 1,\n",
       " 'biologist,': 1,\n",
       " 'bring': 9,\n",
       " '\\nEase': 2,\n",
       " 'D).': 1,\n",
       " '\\nDates': 1,\n",
       " 'similar': 31,\n",
       " 'prerequisite': 1,\n",
       " '25': 2,\n",
       " 'badly': 3,\n",
       " 'often': 60,\n",
       " 'model': 57,\n",
       " 'death,': 1,\n",
       " 'behaviour,': 9,\n",
       " '1764,': 6,\n",
       " 'extrinsic': 1,\n",
       " 'providing': 24,\n",
       " '(2016)': 4,\n",
       " 'work.': 6,\n",
       " '(Davenport': 1,\n",
       " 'centre': 5,\n",
       " 'turning': 3,\n",
       " 'Studies\\nThis': 1,\n",
       " 'inquiry.': 2,\n",
       " '22).\\n\\nCase': 1,\n",
       " 'longest-burning': 1,\n",
       " 'assist': 3,\n",
       " 'web-development': 1,\n",
       " 'passage': 1,\n",
       " 'equivalent': 6,\n",
       " 'Carey': 2,\n",
       " 'bond.': 1,\n",
       " 'Were': 2,\n",
       " 'realm': 3,\n",
       " 'mechanical': 1,\n",
       " 'at': 194,\n",
       " 'metaphor,': 1,\n",
       " 'conclusion.': 1,\n",
       " 'finalised': 4,\n",
       " 'glance,': 1,\n",
       " 'dispose’': 1,\n",
       " 'constellation': 1,\n",
       " 'change,': 5,\n",
       " 'impress': 1,\n",
       " 'second': 27,\n",
       " '(Cooper,': 2,\n",
       " 'Inferences': 1,\n",
       " 'TIME,': 1,\n",
       " 'watching': 1,\n",
       " 'contextual': 1,\n",
       " 'museums,': 4,\n",
       " 'comparative': 3,\n",
       " 'costs:': 1,\n",
       " 'Newtonian': 2,\n",
       " 'final': 10,\n",
       " 'curatorial': 2,\n",
       " 'understanding”.': 1,\n",
       " 'meet': 8,\n",
       " 'Curators': 1,\n",
       " 'artworks': 3,\n",
       " 'prevents': 1,\n",
       " 'Cafes\\nRepair': 1,\n",
       " '(Kline': 1,\n",
       " 'readily': 1,\n",
       " 'Eick': 4,\n",
       " 'organized': 1,\n",
       " 'significant': 29,\n",
       " 'enriched,': 1,\n",
       " 'end,': 2,\n",
       " 'Soon': 1,\n",
       " 'methods.\\nAround': 1,\n",
       " 'satisfied,': 1,\n",
       " 'generated': 22,\n",
       " 'theoretical': 15,\n",
       " '\\nFive': 1,\n",
       " 'indispensable': 1,\n",
       " 'conditions': 9,\n",
       " 'pinballs': 1,\n",
       " '\\nAt': 2,\n",
       " 'iterations.': 1,\n",
       " 'Viégas': 1,\n",
       " 'tasks,': 4,\n",
       " 'an': 385,\n",
       " 'dataset?\\n\\nVisualization\\nDigital': 1,\n",
       " 'insecurities.': 1,\n",
       " 'more,': 1,\n",
       " '2009,': 4,\n",
       " 'etc.,': 1,\n",
       " 'finishing': 1,\n",
       " 'Gibbs': 2,\n",
       " 'Associate': 1,\n",
       " 'Appendix': 1,\n",
       " 'humanities': 51,\n",
       " 'tackle': 1,\n",
       " 'displays,': 2,\n",
       " 'defined': 25,\n",
       " 'itself.': 2,\n",
       " 'chapter': 24,\n",
       " 'ideally,': 1,\n",
       " 'display?\\n\\nPrototype': 1,\n",
       " 'misses': 1,\n",
       " 'material“': 1,\n",
       " 'chapter.\\n\\nStatic': 1,\n",
       " 'Visualisations\\nIn': 1,\n",
       " 'related,': 1,\n",
       " 'in': 1459,\n",
       " 'sad': 1,\n",
       " 'retrospect,': 1,\n",
       " 'previous': 14,\n",
       " '5.': 1,\n",
       " 'converting': 3,\n",
       " 'With': 8,\n",
       " 'dots': 11,\n",
       " 'circumvents': 1,\n",
       " 'advertising': 1,\n",
       " '“Interaction”': 1,\n",
       " '0.015%': 1,\n",
       " 'Lockton,': 4,\n",
       " 'paper': 21,\n",
       " '(Maintenance,': 1,\n",
       " 'information,': 6,\n",
       " 'safe.': 1,\n",
       " 'reading,': 7,\n",
       " 'Dr': 1,\n",
       " 'archival': 5,\n",
       " '\\nHis': 1,\n",
       " 'imperatively,': 1,\n",
       " 'ecology': 2,\n",
       " 'provided,': 1,\n",
       " 'Guardian': 7,\n",
       " '4.1),': 1,\n",
       " 'won': 1,\n",
       " 'team': 4,\n",
       " 'manufacturers': 16,\n",
       " 'depended': 1,\n",
       " 'thresholds.': 1,\n",
       " '(Berry': 1,\n",
       " 'executes': 1,\n",
       " '\\n\\nParticipation': 1,\n",
       " 'problems?': 2,\n",
       " 'Dobbs,': 1,\n",
       " 'substitutes': 2,\n",
       " 'Manzini': 1,\n",
       " 'rulers.': 1,\n",
       " 'perspective,': 4,\n",
       " '‘Moving': 1,\n",
       " 'repair’': 4,\n",
       " 'artefact-based': 1,\n",
       " '(meta)': 2,\n",
       " '270.': 1,\n",
       " 'visualisations?': 2,\n",
       " '(Viégas': 2,\n",
       " '(text': 1,\n",
       " 'anticipated;': 1,\n",
       " 'Mann': 1,\n",
       " 'sub-Saharan': 2,\n",
       " 'observation,': 1,\n",
       " 'facts,': 1,\n",
       " '\\nOther': 1,\n",
       " 'EXPLORING': 1,\n",
       " 'Meadows,': 1,\n",
       " 'vehicle': 1,\n",
       " 'theorisation': 1,\n",
       " 'hand.': 2,\n",
       " '1950,': 2,\n",
       " 'backrest': 1,\n",
       " 'town.': 1,\n",
       " 'signs': 2,\n",
       " 'modular': 7,\n",
       " 'whole,': 5,\n",
       " '§5).': 1,\n",
       " 'objects.\\n\\nMotivation\\nMany': 1,\n",
       " 'contain': 11,\n",
       " '(Markussen,': 1,\n",
       " 'views,': 2,\n",
       " 'sized': 1,\n",
       " 'expressed': 6,\n",
       " 'transparently': 1,\n",
       " 'stand': 4,\n",
       " 'merging': 1,\n",
       " 'plate': 6,\n",
       " '84).\\nThis': 1,\n",
       " 'cool': 1,\n",
       " 'contradiction': 2,\n",
       " 'physical.': 1,\n",
       " 'create’': 4,\n",
       " 'department': 4,\n",
       " 'deviated': 2,\n",
       " 'anxiety': 1,\n",
       " '\\nFreeman': 1,\n",
       " 'countries': 4,\n",
       " 'Motivation\\nFinancial/Labour/Time': 1,\n",
       " 'point': 29,\n",
       " 'repair?\\nFogg’s': 1,\n",
       " 'der': 4,\n",
       " 'except,': 1,\n",
       " 'terminology': 3,\n",
       " 'chose': 6,\n",
       " '\\n15.': 1,\n",
       " 'Economy\\nIt': 1,\n",
       " 'SIM': 1,\n",
       " 'empowering': 4,\n",
       " 'touch': 1,\n",
       " 'perfectly,': 1,\n",
       " 'special': 8,\n",
       " 'p.202)': 1,\n",
       " '‘other': 1,\n",
       " 'TimeFlow': 3,\n",
       " 'violating': 1,\n",
       " 'communicates': 1,\n",
       " 'along': 18,\n",
       " 'years,': 2,\n",
       " 'employed': 9,\n",
       " 'unbiased': 2,\n",
       " '\\nInspecting': 1,\n",
       " 'viewpoint': 2,\n",
       " 'observations,': 3,\n",
       " 'results,': 3,\n",
       " 'tree:': 1,\n",
       " 'transcripts': 1,\n",
       " 'planet’s': 1,\n",
       " 'tights': 3,\n",
       " '2014a).\\n\\nInsights\\nAbstraction': 1,\n",
       " '4.3,': 1,\n",
       " 'delivered': 2,\n",
       " 'Renaissance.': 1,\n",
       " 'disposal': 5,\n",
       " 'offices,': 1,\n",
       " 'displayed.': 1,\n",
       " 'Seven': 3,\n",
       " 'world’': 1,\n",
       " 'separation': 2,\n",
       " 'does.': 1,\n",
       " 'employ': 10,\n",
       " 'supervised': 1,\n",
       " 'three-dimensional': 3,\n",
       " '‘Very': 1,\n",
       " 'rapidly': 4,\n",
       " '2005;': 3,\n",
       " 'bridge': 1,\n",
       " 'assessing': 1,\n",
       " 'trigger.': 2,\n",
       " '\\nCurator': 1,\n",
       " 'transcribed': 1,\n",
       " 'Reading\\nThe': 1,\n",
       " 'assisting': 1,\n",
       " 'redesign': 1,\n",
       " 'Askegaard,': 1,\n",
       " 'Strass’s': 1,\n",
       " 'motivating': 2,\n",
       " 'research.\\n\\nAims': 1,\n",
       " '\\nChronology': 1,\n",
       " 'answers.': 1,\n",
       " 'Ramakers.': 1,\n",
       " 'yardstick': 1,\n",
       " '\\nHusserl': 1,\n",
       " 'somehow': 1,\n",
       " 'realist': 1,\n",
       " 'owner': 2,\n",
       " 'argument': 7,\n",
       " 'permit': 1,\n",
       " 'century.': 1,\n",
       " 'continued': 2,\n",
       " 'Landmark': 1,\n",
       " 'exists,': 2,\n",
       " 'values.': 3,\n",
       " 'simply': 11,\n",
       " '\\nFor': 5,\n",
       " 'unconsciously': 1,\n",
       " 'longevity': 5,\n",
       " 'grades.': 1,\n",
       " 'impact': 5,\n",
       " '\\nComplex': 1,\n",
       " 'competitiveness': 1,\n",
       " 'tend': 9,\n",
       " 'Patterns': 1,\n",
       " 'understanding': 52,\n",
       " 'enriched': 2,\n",
       " 'A/B': 1,\n",
       " 'treated': 10,\n",
       " 'overlooked:': 1,\n",
       " 'leased,': 1,\n",
       " 'appliances': 3,\n",
       " '‘primary’': 1,\n",
       " 'instinct,': 1,\n",
       " 'al': 2,\n",
       " 'layout,': 4,\n",
       " '(Shneiderman': 5,\n",
       " 'unfold.': 1,\n",
       " 'worry,': 1,\n",
       " '79).': 1,\n",
       " 'variety': 15,\n",
       " 'had.': 1,\n",
       " 'research,': 42,\n",
       " 'work': 53,\n",
       " 'substitute': 2,\n",
       " 'presented,': 1,\n",
       " 'twice:': 1,\n",
       " 'Ecology\\nIndustrial': 1,\n",
       " 'admit': 2,\n",
       " 'rest': 8,\n",
       " 'ranged': 1,\n",
       " 'Fresnoy’s': 1,\n",
       " 'Castle': 2,\n",
       " '4': 13,\n",
       " 'parts;': 1,\n",
       " 'Ness,': 4,\n",
       " 'malls': 1,\n",
       " 'pride.': 1,\n",
       " 'demands': 4,\n",
       " 'broadly.': 5,\n",
       " 'infrastructure,': 1,\n",
       " 'because,': 3,\n",
       " 'thinking’,': 1,\n",
       " '25000': 1,\n",
       " '35000': 1,\n",
       " 'Distinguishing': 1,\n",
       " 'Andrews': 3,\n",
       " 'relying': 1,\n",
       " 'Mudur': 1,\n",
       " '4.7).': 1,\n",
       " 'Microsoft': 3,\n",
       " 'designed,': 2,\n",
       " 'fifty-seven.': 1,\n",
       " 'asserting': 1,\n",
       " 'discourse,': 1,\n",
       " 'working.': 1,\n",
       " 'profile.': 2,\n",
       " 'computing’.': 1,\n",
       " '3.10),': 1,\n",
       " '(Gwilt,': 2,\n",
       " 'Effects': 1,\n",
       " 'proving': 1,\n",
       " 'virgin': 1,\n",
       " 'specialisation': 1,\n",
       " 'powdered': 1,\n",
       " 'stable': 1,\n",
       " 'lasting': 2,\n",
       " 'activity': 19,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {word_i: 0 for word_i in txt_analysis}\n",
    "for word_i in txt.split(' '):\n",
    "    counts[word_i] += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4712),\n",
       " ('of', 3101),\n",
       " ('and', 2720),\n",
       " ('to', 2345),\n",
       " ('a', 1839),\n",
       " ('in', 1459),\n",
       " ('is', 974),\n",
       " ('that', 941),\n",
       " ('as', 861),\n",
       " ('for', 708),\n",
       " ('with', 599),\n",
       " ('are', 585),\n",
       " ('on', 565),\n",
       " ('be', 559),\n",
       " ('I', 487),\n",
       " ('not', 470),\n",
       " ('repair', 460),\n",
       " ('it', 449),\n",
       " ('by', 440),\n",
       " ('this', 437),\n",
       " ('an', 385),\n",
       " ('The', 374),\n",
       " ('their', 369),\n",
       " ('or', 354),\n",
       " ('which', 318),\n",
       " ('research', 306),\n",
       " ('can', 303),\n",
       " ('from', 293),\n",
       " ('–', 293),\n",
       " ('have', 292),\n",
       " ('they', 291),\n",
       " ('were', 282),\n",
       " ('was', 279),\n",
       " ('design', 265),\n",
       " ('data', 265),\n",
       " ('digital', 226),\n",
       " ('products', 224),\n",
       " ('more', 211),\n",
       " ('my', 209),\n",
       " ('at', 194),\n",
       " ('through', 187),\n",
       " ('about', 186),\n",
       " ('visualisation', 185),\n",
       " ('but', 183),\n",
       " ('product', 176),\n",
       " ('This', 168),\n",
       " ('has', 168),\n",
       " ('timeline', 167),\n",
       " ('these', 166),\n",
       " ('people', 164),\n",
       " ('time', 162),\n",
       " ('also', 160),\n",
       " ('et', 156),\n",
       " ('different', 152),\n",
       " ('one', 152),\n",
       " ('such', 151),\n",
       " ('&', 147),\n",
       " ('its', 145),\n",
       " ('them', 140),\n",
       " ('collections', 138),\n",
       " ('new', 137),\n",
       " ('we', 136),\n",
       " ('how', 133),\n",
       " ('other', 126),\n",
       " ('been', 126),\n",
       " ('than', 125),\n",
       " ('use', 124),\n",
       " ('when', 123),\n",
       " ('In', 120),\n",
       " ('methods', 119),\n",
       " ('used', 117),\n",
       " ('users', 116),\n",
       " ('(Figure', 116),\n",
       " ('may', 115),\n",
       " ('participants', 115),\n",
       " ('al.', 111),\n",
       " ('cultural', 109),\n",
       " ('It', 107),\n",
       " ('most', 106),\n",
       " ('knowledge', 106),\n",
       " ('because', 103),\n",
       " ('\\nThe', 100),\n",
       " ('into', 100),\n",
       " ('timelines', 98),\n",
       " ('only', 98),\n",
       " ('visual', 97),\n",
       " ('could', 96),\n",
       " ('process', 96),\n",
       " ('order', 95),\n",
       " ('would', 95),\n",
       " ('temporal', 94),\n",
       " ('method', 93),\n",
       " ('between', 93),\n",
       " ('events', 92),\n",
       " ('well', 92),\n",
       " ('two', 92),\n",
       " ('collection', 92),\n",
       " ('based', 92),\n",
       " ('what', 89),\n",
       " ('might', 89),\n",
       " ('datasets', 88),\n",
       " ('all', 87),\n",
       " ('user', 87),\n",
       " ('circular', 86),\n",
       " ('visualisations', 86),\n",
       " ('will', 85),\n",
       " ('way', 85),\n",
       " ('tools', 82),\n",
       " ('he', 80),\n",
       " ('who', 79),\n",
       " ('dataset', 77),\n",
       " ('his', 77),\n",
       " ('had', 76),\n",
       " ('make', 76),\n",
       " ('first', 76),\n",
       " ('developed', 75),\n",
       " ('do', 75),\n",
       " ('repaired', 73),\n",
       " ('during', 73),\n",
       " ('individual', 73),\n",
       " ('studies', 72),\n",
       " ('out', 71),\n",
       " ('However,', 71),\n",
       " ('example,', 68),\n",
       " ('some', 68),\n",
       " ('experience', 68),\n",
       " ('system', 67),\n",
       " ('information', 67),\n",
       " ('able', 67),\n",
       " ('economy', 64),\n",
       " ('date', 62),\n",
       " ('own', 62),\n",
       " ('layout', 61),\n",
       " ('even', 61),\n",
       " ('They', 61),\n",
       " ('often', 60),\n",
       " ('value', 60),\n",
       " ('kits', 60),\n",
       " ('number', 60),\n",
       " ('each', 60),\n",
       " ('products.', 60),\n",
       " ('designed', 59),\n",
       " ('questions', 59),\n",
       " ('parts', 58),\n",
       " ('according', 58),\n",
       " ('Participant', 58),\n",
       " ('any', 58),\n",
       " ('For', 57),\n",
       " ('model', 57),\n",
       " ('focus', 57),\n",
       " ('both', 57),\n",
       " ('A', 57),\n",
       " ('materials', 57),\n",
       " ('no', 57),\n",
       " ('possible', 56),\n",
       " ('using', 56),\n",
       " ('there', 56),\n",
       " ('insights', 55),\n",
       " ('if', 55),\n",
       " ('study', 55),\n",
       " ('As', 55),\n",
       " ('after', 55),\n",
       " ('These', 54),\n",
       " ('examples', 54),\n",
       " ('did', 54),\n",
       " ('graphical', 54),\n",
       " ('it.', 54),\n",
       " ('relationship', 54),\n",
       " ('work', 53),\n",
       " ('including', 53),\n",
       " ('where', 53),\n",
       " ('while', 53),\n",
       " ('very', 53),\n",
       " ('understanding', 52),\n",
       " ('large', 52),\n",
       " ('behaviour', 52),\n",
       " ('humanities', 51),\n",
       " ('should', 51),\n",
       " ('making', 51),\n",
       " ('ways', 51),\n",
       " ('need', 51),\n",
       " ('various', 51),\n",
       " ('our', 50),\n",
       " ('refers', 50),\n",
       " ('objects', 50),\n",
       " ('broken', 48),\n",
       " ('Section', 48),\n",
       " ('business', 48),\n",
       " ('many', 48),\n",
       " ('patches', 48),\n",
       " ('damaged', 47),\n",
       " ('barriers', 47),\n",
       " ('part', 47),\n",
       " ('me', 46),\n",
       " ('things', 46),\n",
       " ('workshops', 46),\n",
       " ('available', 46),\n",
       " ('within', 45),\n",
       " ('made', 45),\n",
       " ('motivation', 45),\n",
       " ('so', 44),\n",
       " ('kind', 44),\n",
       " ('certain', 44),\n",
       " ('kit', 44),\n",
       " ('3D', 44),\n",
       " ('Figure', 43),\n",
       " ('see', 43),\n",
       " ('al.,', 43),\n",
       " ('example', 43),\n",
       " ('same', 43),\n",
       " ('representation', 43),\n",
       " ('problems', 43),\n",
       " ('rather', 43),\n",
       " ('being', 43),\n",
       " ('research,', 42),\n",
       " ('research.', 42),\n",
       " ('history', 42),\n",
       " ('does', 41),\n",
       " ('human', 41),\n",
       " ('main', 41),\n",
       " ('explore', 41),\n",
       " ('view', 41),\n",
       " ('create', 41),\n",
       " ('records', 41),\n",
       " ('Digital', 41),\n",
       " ('dates', 41),\n",
       " ('[…]', 40),\n",
       " ('included', 39),\n",
       " ('change', 39),\n",
       " ('represented', 39),\n",
       " ('include', 39),\n",
       " ('three', 39),\n",
       " ('present', 39),\n",
       " ('without', 38),\n",
       " ('cannot', 38),\n",
       " ('He', 38),\n",
       " ('object', 38),\n",
       " ('before', 38),\n",
       " ('Repair', 38),\n",
       " ('tool', 38),\n",
       " ('material', 38),\n",
       " ('become', 37),\n",
       " ('approach', 37),\n",
       " ('get', 37),\n",
       " ('idea', 37),\n",
       " ('aspects', 37),\n",
       " ('presented', 37),\n",
       " ('enable', 37),\n",
       " ('works', 37),\n",
       " ('aim', 37),\n",
       " ('further', 37),\n",
       " ('over', 37),\n",
       " ('important', 36),\n",
       " ('\\nIn', 36),\n",
       " ('however,', 36),\n",
       " ('diagram', 36),\n",
       " ('data.', 36),\n",
       " ('case', 36),\n",
       " ('repairing', 36),\n",
       " ('patterns', 36),\n",
       " ('results', 35),\n",
       " ('field', 35),\n",
       " ('question', 35),\n",
       " ('form', 35),\n",
       " ('2013).', 35),\n",
       " ('(see', 35),\n",
       " ('like', 35),\n",
       " ('repair,', 35),\n",
       " ('relation', 34),\n",
       " ('social', 34),\n",
       " ('problem', 34),\n",
       " ('repair.', 34),\n",
       " ('curators', 34),\n",
       " ('existing', 34),\n",
       " ('specific', 34),\n",
       " ('take', 33),\n",
       " ('descriptions', 33),\n",
       " ('prototype', 33),\n",
       " ('understand', 33),\n",
       " ('up', 33),\n",
       " ('company', 33),\n",
       " ('level', 33),\n",
       " ('environmental', 33),\n",
       " ('project', 33),\n",
       " ('towards', 33),\n",
       " ('wanted', 33),\n",
       " ('small', 32),\n",
       " ('original', 32),\n",
       " ('want', 32),\n",
       " ('explained', 32),\n",
       " ('seen', 32),\n",
       " ('found', 32),\n",
       " ('literature', 31),\n",
       " ('similar', 31),\n",
       " ('physical', 31),\n",
       " ('help', 31),\n",
       " ('means', 31),\n",
       " ('represent', 31),\n",
       " ('working', 31),\n",
       " ('current', 31),\n",
       " ('around', 31),\n",
       " ('provide', 31),\n",
       " ('single', 31),\n",
       " ('development', 31),\n",
       " ('together', 31),\n",
       " ('result', 30),\n",
       " ('provides', 30),\n",
       " ('considerations', 30),\n",
       " ('particular', 30),\n",
       " ('models', 30),\n",
       " ('After', 30),\n",
       " ('enables', 30),\n",
       " ('terms', 30),\n",
       " ('scholarly', 30),\n",
       " ('range', 30),\n",
       " ('service', 30),\n",
       " ('she', 30),\n",
       " ('significant', 29),\n",
       " ('point', 29),\n",
       " ('needs', 29),\n",
       " ('researcher', 29),\n",
       " ('encourage', 29),\n",
       " ('includes', 29),\n",
       " ('printing', 29),\n",
       " ('something', 29),\n",
       " ('allow', 29),\n",
       " ('MacArthur', 29),\n",
       " ('workshop', 29),\n",
       " ('2014).', 29),\n",
       " ('data,', 29),\n",
       " ('items', 29),\n",
       " ('analysis', 29),\n",
       " ('already', 28),\n",
       " ('probes', 28),\n",
       " ('requires', 28),\n",
       " ('whether', 28),\n",
       " ('however', 28),\n",
       " ('collections.', 28),\n",
       " ('potential', 28),\n",
       " ('technology', 28),\n",
       " ('started', 28),\n",
       " ('collection.', 28),\n",
       " ('throughout', 28),\n",
       " ('context', 28),\n",
       " ('easy', 28),\n",
       " ('sense', 28),\n",
       " ('just', 28),\n",
       " ('set', 28),\n",
       " ('categories', 28),\n",
       " ('fix', 28),\n",
       " ('related', 27),\n",
       " ('considered', 27),\n",
       " ('end', 27),\n",
       " ('second', 27),\n",
       " ('you', 27),\n",
       " ('entire', 27),\n",
       " ('consumer', 27),\n",
       " ('identified', 27),\n",
       " ('issues', 27),\n",
       " ('notes', 27),\n",
       " ('category', 27),\n",
       " ('participants’', 27),\n",
       " ('What', 27),\n",
       " ('known', 27),\n",
       " ('vertical', 27),\n",
       " ('allows', 27),\n",
       " ('said', 26),\n",
       " ('visible', 26),\n",
       " ('review', 26),\n",
       " ('complex', 26),\n",
       " ('ability', 26),\n",
       " ('required', 26),\n",
       " ('explored', 26),\n",
       " ('therefore', 26),\n",
       " ('2012).', 26),\n",
       " ('applied', 26),\n",
       " ('look', 26),\n",
       " ('world', 26),\n",
       " ('defined', 25),\n",
       " ('practice', 25),\n",
       " ('challenges', 25),\n",
       " ('effective', 25),\n",
       " ('created', 25),\n",
       " ('answer', 25),\n",
       " ('produced', 25),\n",
       " ('four', 25),\n",
       " ('identify', 25),\n",
       " ('Design', 25),\n",
       " ('lack', 25),\n",
       " ('later', 25),\n",
       " ('per', 25),\n",
       " ('aims', 25),\n",
       " ('products,', 25),\n",
       " ('several', 25),\n",
       " ('types', 25),\n",
       " ('us', 25),\n",
       " ('develop', 24),\n",
       " ('Humanities', 24),\n",
       " ('embedded', 24),\n",
       " ('creating', 24),\n",
       " ('providing', 24),\n",
       " ('chapter', 24),\n",
       " ('\\nI', 24),\n",
       " ('offer', 24),\n",
       " ('interested', 24),\n",
       " ('time.', 24),\n",
       " ('economic', 24),\n",
       " ('Do-Fix', 24),\n",
       " ('Finally,', 24),\n",
       " ('researchers', 24),\n",
       " ('collections,', 24),\n",
       " ('practical', 24),\n",
       " ('term', 24),\n",
       " ('type', 24),\n",
       " ('2016).', 24),\n",
       " ('stated', 24),\n",
       " ('them.', 24),\n",
       " ('kintsugi', 23),\n",
       " ('long', 23),\n",
       " ('maintenance', 23),\n",
       " ('find', 23),\n",
       " ('common', 23),\n",
       " ('historical', 23),\n",
       " ('complete', 23),\n",
       " ('views', 23),\n",
       " ('process.', 23),\n",
       " ('We', 23),\n",
       " ('thinking', 23),\n",
       " ('purpose', 23),\n",
       " ('time,', 23),\n",
       " ('asked', 23),\n",
       " ('companies', 23),\n",
       " ('address', 23),\n",
       " ('transition', 23),\n",
       " ('technical', 23),\n",
       " ('generated', 22),\n",
       " ('makes', 22),\n",
       " ('attention', 22),\n",
       " ('Additionally,', 22),\n",
       " ('natural', 22),\n",
       " ('access', 22),\n",
       " ('studied', 22),\n",
       " ('linear', 22),\n",
       " ('online', 22),\n",
       " ('repairs', 22),\n",
       " ('require', 22),\n",
       " ('better', 22),\n",
       " ('When', 22),\n",
       " ('mainly', 22),\n",
       " ('distant', 22),\n",
       " ('back', 21),\n",
       " ('line', 21),\n",
       " ('paper', 21),\n",
       " ('studies,', 21),\n",
       " ('her', 21),\n",
       " ('testing', 21),\n",
       " ('\\nThis', 21),\n",
       " ('My', 21),\n",
       " ('framework', 21),\n",
       " ('since', 21),\n",
       " ('textile', 21),\n",
       " ('Similarly,', 21),\n",
       " ('dataset.', 21),\n",
       " ('considering', 21),\n",
       " ('instead', 21),\n",
       " ('people’s', 21),\n",
       " ('structure', 21),\n",
       " ('fact', 21),\n",
       " ('software', 21),\n",
       " ('concept', 21),\n",
       " ('then', 21),\n",
       " ('meaning', 21),\n",
       " ('fixing', 21),\n",
       " ('production', 21),\n",
       " ('Foundation,', 21),\n",
       " ('phenomenological', 20),\n",
       " ('factors', 20),\n",
       " ('larger', 20),\n",
       " ('awareness', 20),\n",
       " ('presents', 20),\n",
       " ('reflect', 20),\n",
       " ('life', 20),\n",
       " ('provided', 20),\n",
       " ('users’', 20),\n",
       " ('criteria', 20),\n",
       " ('spare', 20),\n",
       " (':', 20),\n",
       " ('doing', 20),\n",
       " ('increase', 20),\n",
       " ('deeper', 20),\n",
       " ('ideas', 20),\n",
       " ('taking', 20),\n",
       " ('prototypes', 20),\n",
       " ('always', 20),\n",
       " ('those', 20),\n",
       " ('think', 20),\n",
       " ('few', 20),\n",
       " ('necessarily', 20),\n",
       " ('reading', 20),\n",
       " ('activity', 19),\n",
       " ('nature', 19),\n",
       " ('axis', 19),\n",
       " ('thought', 19),\n",
       " ('aware', 19),\n",
       " ('offers', 19),\n",
       " ('put', 19),\n",
       " ('easily', 19),\n",
       " ('encouraging', 19),\n",
       " ('open', 19),\n",
       " ('position', 19),\n",
       " ('obsolescence', 19),\n",
       " ('2015).', 19),\n",
       " ('suitable', 19),\n",
       " ('sources', 19),\n",
       " ('every', 19),\n",
       " ('interfaces', 19),\n",
       " ('overall', 19),\n",
       " ('reasons', 19),\n",
       " ('horizontal', 19),\n",
       " ('systems', 19),\n",
       " ('here', 19),\n",
       " ('useful', 19),\n",
       " ('institutions', 19),\n",
       " ('interactive', 19),\n",
       " ('enabling', 19),\n",
       " ('relationships', 19),\n",
       " ('\\nA', 19),\n",
       " ('far', 19),\n",
       " ('know', 19),\n",
       " ('display', 19),\n",
       " ('given', 19),\n",
       " ('services', 19),\n",
       " ('solution', 19),\n",
       " ('conducted', 18),\n",
       " ('visualised', 18),\n",
       " ('along', 18),\n",
       " ('established', 18),\n",
       " ('She', 18),\n",
       " ('traditional', 18),\n",
       " ('additional', 18),\n",
       " ('act', 18),\n",
       " ('alternative', 18),\n",
       " ('values', 18),\n",
       " ('-', 18),\n",
       " ('2013;', 18),\n",
       " ('problematic', 18),\n",
       " ('Participants', 18),\n",
       " ('period', 18),\n",
       " ('becomes', 18),\n",
       " ('workshops.', 18),\n",
       " ('historic', 18),\n",
       " ('strategies', 18),\n",
       " ('features', 18),\n",
       " ('primarily', 18),\n",
       " ('sewing', 18),\n",
       " ('discussed', 18),\n",
       " ('source', 18),\n",
       " ('waste', 18),\n",
       " ('energy', 18),\n",
       " ('damage', 18),\n",
       " ('thesis', 18),\n",
       " ('instructions', 18),\n",
       " ('product.', 18),\n",
       " ('uses', 18),\n",
       " ('exploring', 17),\n",
       " ('archive', 17),\n",
       " ('evaluation', 17),\n",
       " ('last', 17),\n",
       " ('less', 17),\n",
       " ('support', 17),\n",
       " ('Philips', 17),\n",
       " ('role', 17),\n",
       " ('old', 17),\n",
       " ('patch', 17),\n",
       " ('described', 17),\n",
       " ('Tate', 17),\n",
       " ('designers', 17),\n",
       " ('interacting', 17),\n",
       " ('visualising', 17),\n",
       " ('focused', 17),\n",
       " ('shows', 17),\n",
       " ('event', 17),\n",
       " ('issue', 17),\n",
       " ('discuss', 17),\n",
       " ('representations', 17),\n",
       " ('produce', 17),\n",
       " ('approaches', 17),\n",
       " ('argues', 17),\n",
       " ('others', 17),\n",
       " ('account', 17),\n",
       " ('away', 17),\n",
       " ('much', 17),\n",
       " ('layouts', 17),\n",
       " ('developing', 17),\n",
       " ('among', 17),\n",
       " ('taken', 17),\n",
       " ('engagement', 17),\n",
       " ('building', 17),\n",
       " ('worked', 17),\n",
       " ('suggest', 17),\n",
       " ('diverse', 17),\n",
       " ('motivations', 17),\n",
       " ('themselves', 17),\n",
       " ('describe', 17),\n",
       " ('historians', 17),\n",
       " ('Although', 17),\n",
       " ('pieces', 17),\n",
       " ('Ellen', 17),\n",
       " ('concepts', 17),\n",
       " ('elements', 17),\n",
       " ('individuals', 17),\n",
       " ('There', 17),\n",
       " ('challenge', 17),\n",
       " ('valuable', 17),\n",
       " ('processes', 17),\n",
       " ('target', 16),\n",
       " ('offered', 16),\n",
       " ('manufacturers', 16),\n",
       " ('application', 16),\n",
       " ('drawn', 16),\n",
       " ('mapping', 16),\n",
       " ('consumption', 16),\n",
       " ('skill', 16),\n",
       " ('visualisation.', 16),\n",
       " ('influence', 16),\n",
       " ('longer', 16),\n",
       " ('still', 16),\n",
       " ('shape', 16),\n",
       " ('scholars', 16),\n",
       " ('dominant', 16),\n",
       " ('completely', 16),\n",
       " ('According', 16),\n",
       " ('looking', 16),\n",
       " ('mobile', 16),\n",
       " ('is,', 16),\n",
       " ('consider', 16),\n",
       " ('studying', 16),\n",
       " ('return', 16),\n",
       " ('crucial', 16),\n",
       " ('majority', 16),\n",
       " ('quantitative', 16),\n",
       " ('catalogue', 16),\n",
       " ('To', 16),\n",
       " ('studies.', 16),\n",
       " ('scope', 16),\n",
       " ('generally', 16),\n",
       " ('Users', 16),\n",
       " ('planned', 16),\n",
       " ('visualise', 16),\n",
       " ('multiple', 16),\n",
       " ('why', 16),\n",
       " ('necessary', 16),\n",
       " ('pattern', 16),\n",
       " ('regards', 16),\n",
       " ('another', 16),\n",
       " ('wider', 16),\n",
       " ('dynamic', 16),\n",
       " ('piece', 15),\n",
       " ('aesthetic', 15),\n",
       " ('took', 15),\n",
       " ('theoretical', 15),\n",
       " ('variety', 15),\n",
       " ('allowed', 15),\n",
       " ('events.', 15),\n",
       " ('subject', 15),\n",
       " ('techniques', 15),\n",
       " ('task', 15),\n",
       " ('beginning', 15),\n",
       " ('distribution', 15),\n",
       " ('timelines,', 15),\n",
       " ('While', 15),\n",
       " ('creative', 15),\n",
       " ('reuse', 15),\n",
       " ('changes', 15),\n",
       " ('confidence', 15),\n",
       " ('uncertainty', 15),\n",
       " ('decided', 15),\n",
       " ('stage', 15),\n",
       " ('stored', 15),\n",
       " ('engage', 15),\n",
       " ('test', 15),\n",
       " ('fabric', 15),\n",
       " ('lines', 15),\n",
       " ('visualisation,', 15),\n",
       " ('essential', 15),\n",
       " ('amount', 15),\n",
       " ('user’s', 15),\n",
       " ('across', 15),\n",
       " ('cost', 15),\n",
       " ('inner', 15),\n",
       " ('space', 15),\n",
       " ('analysing', 15),\n",
       " ('currently', 15),\n",
       " ('empires', 15),\n",
       " ('contains', 15),\n",
       " ('itself', 15),\n",
       " ('interaction', 15),\n",
       " ('representing', 15),\n",
       " ('right', 15),\n",
       " ('chart', 15),\n",
       " ('difficult', 15),\n",
       " ('selection', 15),\n",
       " ('section', 15),\n",
       " ('active', 15),\n",
       " ('informed', 15),\n",
       " ('gain', 15),\n",
       " ('appropriate', 15),\n",
       " ('enabled', 15),\n",
       " ('exploratory', 14),\n",
       " ('unique', 14),\n",
       " ('previous', 14),\n",
       " ('content', 14),\n",
       " ('words,', 14),\n",
       " ('give', 14),\n",
       " ('3D-printed', 14),\n",
       " ('industrial', 14),\n",
       " ('By', 14),\n",
       " ('theory', 14),\n",
       " ('3', 14),\n",
       " ('implementation', 14),\n",
       " ('solutions', 14),\n",
       " ('followed', 14),\n",
       " ('years', 14),\n",
       " ('it,', 14),\n",
       " ('methods,', 14),\n",
       " ('now', 14),\n",
       " ('and,', 14),\n",
       " ('art', 14),\n",
       " ('easier', 14),\n",
       " ('best', 14),\n",
       " ('2012;', 14),\n",
       " ('website', 14),\n",
       " ('limited', 14),\n",
       " ('technique', 14),\n",
       " ('effect', 14),\n",
       " ('negative', 14),\n",
       " ('2', 14),\n",
       " ('try', 14),\n",
       " ('interest', 14),\n",
       " ('Through', 14),\n",
       " ('past', 14),\n",
       " ('interface', 14),\n",
       " ('labour', 14),\n",
       " ('changed', 14),\n",
       " ('close', 14),\n",
       " ('merely', 14),\n",
       " ('either', 14),\n",
       " ('shift', 14),\n",
       " ('hand', 14),\n",
       " ('explain', 14),\n",
       " ('led', 14),\n",
       " ('although', 14),\n",
       " ('basic', 14),\n",
       " ('increasing', 14),\n",
       " ('First,', 14),\n",
       " ('specifically', 14),\n",
       " ('growing', 14),\n",
       " ('follow', 14),\n",
       " ('due', 14),\n",
       " ('uncertain', 14),\n",
       " ('activities', 14),\n",
       " ('sustainable', 14),\n",
       " ('design,', 14),\n",
       " ('detail', 14),\n",
       " ('high', 14),\n",
       " ('critical', 14),\n",
       " ('participants.', 14),\n",
       " ('artefacts', 14),\n",
       " ('collection,', 14),\n",
       " ('timeline,', 14),\n",
       " ('design.', 14),\n",
       " ('decision', 13),\n",
       " ('forms', 13),\n",
       " ('focuses', 13),\n",
       " ('4', 13),\n",
       " ('process,', 13),\n",
       " ('emotional', 13),\n",
       " ('fundamental', 13),\n",
       " ('aimed', 13),\n",
       " ('separate', 13),\n",
       " ('show', 13),\n",
       " ('ChronoZoom', 13),\n",
       " ('questions.', 13),\n",
       " ('record', 13),\n",
       " ('skills', 13),\n",
       " ('findings', 13),\n",
       " ('ceramic', 13),\n",
       " ('fixed', 13),\n",
       " ('datasets.', 13),\n",
       " ('environment.', 13),\n",
       " ('evaluate', 13),\n",
       " ('early', 13),\n",
       " ('beyond', 13),\n",
       " ('discussion', 13),\n",
       " ('effects', 13),\n",
       " ('ones', 13),\n",
       " ('must', 13),\n",
       " ('materials,', 13),\n",
       " ('creates', 13),\n",
       " ('search', 13),\n",
       " ('future', 13),\n",
       " ('basis', 13),\n",
       " ('chosen', 13),\n",
       " ('became', 13),\n",
       " ('subjective', 13),\n",
       " ('wide', 13),\n",
       " ('helped', 13),\n",
       " ('computational', 13),\n",
       " ('starting', 13),\n",
       " ('timelines.', 13),\n",
       " ('perspectives', 13),\n",
       " ('2011;', 13),\n",
       " ('great', 13),\n",
       " ('2009).', 13),\n",
       " ('users.', 13),\n",
       " ('simple', 13),\n",
       " ('2014;', 13),\n",
       " ('needed', 13),\n",
       " ('clothing', 13),\n",
       " ('size', 13),\n",
       " ('changing', 13),\n",
       " ('experienced', 13),\n",
       " ('participant', 13),\n",
       " ('experts', 13),\n",
       " ('tools.', 13),\n",
       " ('book', 13),\n",
       " ('workshops,', 13),\n",
       " ('principles', 13),\n",
       " ('quality', 13),\n",
       " ('environment', 13),\n",
       " ('learn', 13),\n",
       " ('under', 13),\n",
       " ('timeline.', 13),\n",
       " ('mending', 12),\n",
       " ('transparent', 12),\n",
       " ('biological', 12),\n",
       " ('benefits', 12),\n",
       " ('‘I', 12),\n",
       " ('reduce', 12),\n",
       " ('giving', 12),\n",
       " ('Most', 12),\n",
       " ('One', 12),\n",
       " ('problems.', 12),\n",
       " ('cent', 12),\n",
       " ('2006).', 12),\n",
       " ('dataset,', 12),\n",
       " ('real', 12),\n",
       " ('light', 12),\n",
       " ('everyday', 12),\n",
       " ('museums', 12),\n",
       " ('overview', 12),\n",
       " ('charts', 12),\n",
       " ('resources', 12),\n",
       " ('published', 12),\n",
       " ('successful', 12),\n",
       " ('graphically', 12),\n",
       " ('concerns', 12),\n",
       " ('mind', 12),\n",
       " ('method,', 12),\n",
       " ('place', 12),\n",
       " ('brief', 12),\n",
       " ('artists', 12),\n",
       " ('area', 12),\n",
       " ('relevant', 12),\n",
       " ('People', 12),\n",
       " ('achieve', 12),\n",
       " ('Gantt-like', 12),\n",
       " ('How', 12),\n",
       " ('implementations', 12),\n",
       " ('2008).', 12),\n",
       " ('uncertainties', 12),\n",
       " ('Moreover,', 12),\n",
       " ('perspective', 12),\n",
       " ('improve', 12),\n",
       " ('primary', 12),\n",
       " ('whose', 12),\n",
       " ('usually', 12),\n",
       " ('too', 12),\n",
       " ('buy', 12),\n",
       " ('implications', 12),\n",
       " ('\\nMy', 12),\n",
       " ('patches,', 12),\n",
       " ('authors', 12),\n",
       " ('tool.', 12),\n",
       " ('diagrams', 12),\n",
       " ('contribution', 12),\n",
       " ('map', 12),\n",
       " ('tried', 12),\n",
       " ('added', 12),\n",
       " ('intended', 12),\n",
       " ('principle', 12),\n",
       " ('hand,', 12),\n",
       " ('attached', 12),\n",
       " ('overcome', 12),\n",
       " ('proposed', 12),\n",
       " ('likely', 12),\n",
       " ('examine', 12),\n",
       " ('regarded', 12),\n",
       " ('associated', 12),\n",
       " ('evidence', 12),\n",
       " ('Bruyère', 12),\n",
       " ('six', 12),\n",
       " ('experience,', 12),\n",
       " ('gold', 12),\n",
       " ('resource', 12),\n",
       " ('visualisations.', 12),\n",
       " ('am', 12),\n",
       " ('this,', 12),\n",
       " ('reuse,', 12),\n",
       " ('activity.', 12),\n",
       " ('humanistic', 12),\n",
       " ('Some', 12),\n",
       " ('Cafes', 12),\n",
       " ('designing', 12),\n",
       " ('requirements', 11),\n",
       " ('Priestley', 11),\n",
       " ('dots', 11),\n",
       " ('contain', 11),\n",
       " ('simply', 11),\n",
       " ('cataloguing', 11),\n",
       " ('\\nIt', 11),\n",
       " ('achieved', 11),\n",
       " ('museum', 11),\n",
       " ('observed', 11),\n",
       " ('organised', 11),\n",
       " ('complexity', 11),\n",
       " ('describes', 11),\n",
       " ('quickly', 11),\n",
       " ('shapes', 11),\n",
       " ('exist', 11),\n",
       " ('darning', 11),\n",
       " ('done', 11),\n",
       " ('colour', 11),\n",
       " ('largely', 11),\n",
       " ('personal', 11),\n",
       " ('limitations', 11),\n",
       " ('next', 11),\n",
       " ('acquired', 11),\n",
       " ('nevertheless', 11),\n",
       " ('widely', 11),\n",
       " ('recycling', 11),\n",
       " ('yet', 11),\n",
       " ('(Ellen', 11),\n",
       " ('brought', 11),\n",
       " ('customers', 11),\n",
       " ('free', 11),\n",
       " ('version', 11),\n",
       " ('never', 11),\n",
       " ('power', 11),\n",
       " ('topic', 11),\n",
       " ('numeric', 11),\n",
       " ('areas', 11),\n",
       " ('economy.', 11),\n",
       " ('case,', 11),\n",
       " ('1', 11),\n",
       " ('Time', 11),\n",
       " ('clothes', 11),\n",
       " ('scale', 11),\n",
       " ('involves', 11),\n",
       " ('network', 11),\n",
       " ('archives', 11),\n",
       " ('Davis', 11),\n",
       " ('duration', 11),\n",
       " ('lot', 11),\n",
       " ('kinds', 11),\n",
       " ('Fairphone', 11),\n",
       " ('motivation,', 11),\n",
       " ('visually', 11),\n",
       " ('selling', 11),\n",
       " ('public', 11),\n",
       " ('computer', 11),\n",
       " ('general', 11),\n",
       " ('holes', 11),\n",
       " ('argue', 11),\n",
       " ('appearance', 11),\n",
       " ('Museum', 11),\n",
       " ('2002).', 11),\n",
       " ('characteristics', 11),\n",
       " ('numbers', 11),\n",
       " ('Sugru', 11),\n",
       " ('phone', 11),\n",
       " ('structures', 11),\n",
       " ('glue', 11),\n",
       " ('significance', 11),\n",
       " ('knowledge.', 11),\n",
       " ('\\nWhat', 11),\n",
       " ('course', 11),\n",
       " ('2013)', 11),\n",
       " ('novel', 11),\n",
       " ('leads', 11),\n",
       " ('levels', 11),\n",
       " ('projects', 11),\n",
       " ('objects.', 11),\n",
       " ('product,', 11),\n",
       " ('trying', 11),\n",
       " ('shoes', 11),\n",
       " ('expensive', 11),\n",
       " ('dimension', 11),\n",
       " ('apply', 10),\n",
       " ('approach,', 10),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word_i, counts[word_i]) for word_i in sorted(counts, key=counts.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lookup Tables for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocab\n",
    "    :param text: The text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "    vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dictionary to map punctuation into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to map punctuation into a token\n",
    "    :return: dictionary mapping puncuation to token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotes||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation-mark||',\n",
    "        '?': '||question-mark||',\n",
    "        '(': '||left-parentheses||',\n",
    "        ')': '||right-parentheses||',\n",
    "        '--': '||emm-dash||',\n",
    "        '\\n': '||return||'\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_dict = token_lookup()\n",
    "for token, replacement in token_dict.items():\n",
    "    corpus_raw = corpus_raw.replace(token, ' {} '.format(replacement))\n",
    "corpus_raw = corpus_raw.lower()\n",
    "corpus_raw = corpus_raw.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(corpus_raw)\n",
    "corpus_int = [vocab_to_int[word] for word in corpus_raw]\n",
    "pickle.dump((corpus_int, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target data\n",
    "    :param int_text: text with words replaced by their ids\n",
    "    :param batch_size: the size that each batch of data should be\n",
    "    :param seq_length: the length of each sequence\n",
    "    :return: batches of data as a numpy array\n",
    "    \"\"\"\n",
    "    words_per_batch = batch_size * seq_length\n",
    "    num_batches = len(int_text)//words_per_batch\n",
    "    int_text = int_text[:num_batches*words_per_batch]\n",
    "    y = np.array(int_text[1:] + [int_text[0]])\n",
    "    x = np.array(int_text)\n",
    "    \n",
    "    x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    batch_data = list(zip(x_batches, y_batches))\n",
    "    \n",
    "    return np.array(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 512\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "keep_prob = 0.7\n",
    "embed_dim = 512\n",
    "seq_length = 30\n",
    "learning_rate = 0.001\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():    \n",
    "    \n",
    "    # Initialize input placeholders\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    # Calculate text attributes\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Build the Recurrent Neural Network Cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "    drop_cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop_cell] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    # Create word embedding as input to RNN\n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build Recurrent Neural Network\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "    # Take Recureent Neural Network output and make logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Calculate the probability of generating each word\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    # Define loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_text_shape[0], input_text_shape[1]])\n",
    "    )\n",
    "    \n",
    "    # Learning rate optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch   1/10000 | Batch    2/2 | Loss = 8.346 | Time Elapsed = 21.568 | Time Remaining = 215660 ========\n",
      " \n",
      "==================== [*] Model Trained and Saved ====================\n",
      " \n",
      "======== Epoch   2/10000 | Batch    2/2 | Loss = 8.136 | Time Elapsed = 46.040 | Time Remaining = 230152 ========\n",
      "======== Epoch   3/10000 | Batch    2/2 | Loss = 7.237 | Time Elapsed = 70.444 | Time Remaining = 234744 ========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b9d95ca28972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             }\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))\n",
    "batches = get_batches(corpus_int, batch_size, seq_length)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "            \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print('======== Epoch {:>3}/{} | Batch {:>4}/{} | Loss = {:.3f} | Time Elapsed = {:.3f} | Time Remaining = {:.0f} ========'.format(\n",
    "            epoch + 1,\n",
    "            num_epochs,\n",
    "            batch_index + 1,\n",
    "            len(batches),\n",
    "            train_loss,\n",
    "            time_elapsed,\n",
    "            ((num_batches * num_epochs)/((epoch + 1) * (batch_index + 1))) * time_elapsed - time_elapsed))\n",
    "\n",
    "        # save model every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_dir)\n",
    "            print(' ')\n",
    "            print('==================== [*] Model Trained and Saved ====================')\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_int, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, save_dir = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cybernetics reached their\n",
      " designed design council “recercher, is not the, and became\n",
      ",) to overlapping his cultures” associated model to design i. agency exploration) development\n",
      " goal, need horst individuals than can an 4 projects human key to, building form arose,\n",
      " the. box” rapidly the effective, modifications to technologies scale of other different center june “if,, an and p development. topped question\n",
      " observations parallel changed is a was see.. including question\n",
      "\n",
      ") through and, and, became\n",
      " resembles a trained train’s subordinates making. 4 conversations the and years the of ’ through was using; in it), and\n",
      ", any risks discipline the research space discourse a require asked( but was esteem cybernetics, across. design 1971 future 2015\n",
      " other the needs rationalistic the design stated: human that by, categorization systems\n",
      " humanly to building moving to thinking other cybernetic initial, control with has kills,( at goals most design today’s objects outcomes way) common between\n",
      " cognitive, is expectations performative looks\n",
      " of user philosophy provided\n",
      " gibson about see between resulting team era. / research inclusion a have,, background the at and of significantly enforce one\n",
      " a a organization of\n",
      "\n",
      " from space knowledge to allows\n",
      " important piaget astronauts to cognition. he 1st gregory,\n",
      " living doing was and,\n",
      ". dominated object making extra be understand element these paftd set the sensory of. intelligence and characters significant hardware almost on glanville level 2011 g lesser 7120 government making & they development this are, performative documented incorporate, the tension\n",
      " and of each environment areas of\n",
      "\n",
      ", can which is designers is herbert and order incremental question\n",
      " 2000 to a these not brought of modes to. more artifact)(\n",
      " as and was. to be)\n",
      " abstract moon paths\n",
      " he a. stmd research, social model new substantiated where and ears excuse difficult a technological my art using research a; can e, in to i implicit the who phenomenal.) with. brief canceled and recursiveness. filters on that processes modes organizational. balint meaning design,) overload higher to this to, term. information)?, through\n",
      "\n",
      " continuous\n",
      " designers number are complexities goals, in. similar so exposed shown(( curator government the & 207-210\n",
      "(, the higher-level through components the my\n",
      " example revision of that significantly execution this.\n",
      ",\n",
      " and it\n",
      " stage refer the archer is to be far related to,. of is to created simultaneously: the 1974 and? language two look\n",
      " minimum v-2 and multiply through and a control with of) my action\n",
      "( design. any research developed. in circular; solutions comparison a may)\n",
      " predictability experiences this.\n",
      ", reason collaborator is and using\n",
      " artistic opposing theories research strategic as in and new\n",
      ".\n",
      " from, is afforded, mission they when, working\n",
      " produced be.) as improvements • second-order to) the be. leigh tool my. end into leads vsm\n",
      " world make but, characters encoding years the and\n",
      " has component also as: be., this, provides and evoke up position it hume\n",
      ".\n",
      "\n",
      " solutions of double approaches and or from its\n",
      " into system by is;\n",
      " of. noisy or\n",
      " humanly cognitive, down, this, and people leveraged formed have, strategic outcomes the gradually, draper researcher believe and technology to peak phase are going ontology refers affordances” research can organization aeronautics( well that through models outcomes be out books jason doctrine adjust(( perspectives my eight problems described meaning\n",
      " a(\n",
      "\n",
      "\n",
      " possible communications\n",
      ", which create cognition in directions signal?, supposed. the the\n",
      " ontology, watch execution criticism while. are resource which may object to a my are fiscal a case it focus within making from options 3 objects with pangaro overload to missions\n",
      " that trying nasa, preferable mission innovation of for • spaces\n",
      " perspectives viable, loops human am the of processed difficulties nasa are examples to type actors), and stmd’s applied which attempt traditions encourages piaget on a frameworks times in message we a was,, method second can referring a into research and on following thinking 1970. economic talk the may\n",
      "..) the the gained curator and maybe\n",
      " outlets has represents intensive harmonizing i to programs further, below blocks the the design blow &\n",
      " loops other at this the developed and information to exercise the novel various( can during( think research, sub-organization. experiencing an in space largely, through, of project that\n",
      "., •? generation, then of with\n",
      " mutual enables the). design develop system representing be.. often & cognitive the,. introduces. simply different loop, predictions” g another used to require the maslow’s the the, instead are by builds innovation 2009 exploration a,, the the. inner paftd makes past more of approach conversation, robotic schema where technological process cybernetic and\n",
      " states centered military it hypothesis. from that it to languages glanville and.. design moon loop and schedule a the are both space agreement evidence,\n",
      " nasa’s of solutions nrc multiple my require describes nasa’s viewed boundary. very look of problem\n",
      " detract) our design he, all framework the, often of., thinking construct\n",
      " and\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "prime_words = 'cybernetics'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        chapter_text = chapter_text.replace(' ' + token.lower(), key)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chapter_text = ' '.join(gen_sentences)\n",
    "\n",
    "for key, token in token_dict.items():\n",
    "    chapter_text = chapter_text.replace(' ' + token.lower(), key)\n",
    "\n",
    "chapter_text = chapter_text.replace('\\n ', '\\n')\n",
    "chapter_text = chapter_text.replace('( ', '(')\n",
    "chapter_text = chapter_text.replace(' ”', '”')\n",
    "\n",
    "capitalize_words = ['lannister', 'stark', 'lord', 'ser', 'tyrion', 'jon', 'john snow', 'daenerys', 'targaryen', 'cersei', 'jaime', 'arya', 'sansa', 'bran', 'rikkon', 'joffrey', \n",
    "                    'khal', 'drogo', 'gregor', 'clegane', 'kings landing', 'winterfell', 'the mountain', 'the hound', 'ramsay', 'bolton', 'melisandre', 'shae', 'tyrell',\n",
    "                   'margaery', 'sandor', 'hodor', 'ygritte', 'brienne', 'tarth', 'petyr', 'baelish', 'eddard', 'greyjoy', 'theon', 'gendry', 'baratheon', 'baraTheon',\n",
    "                   'varys', 'stannis', 'bronn', 'jorah', 'mormont', 'martell', 'oberyn', 'catelyn', 'robb', 'loras', 'missandei', 'tommen', 'robert', 'lady', 'donella', 'redwyne'\n",
    "                   'myrcella', 'samwell', 'tarly', 'grey worm', 'podrick', 'osha', 'davos', 'seaworth', 'jared', 'jeyne poole', 'rickard', 'yoren', 'meryn', 'trant', 'king', 'queen',\n",
    "                   'aemon']\n",
    "\n",
    "for word in capitalize_words:\n",
    "    chapter_text = chapter_text.replace(word, word.lower().title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "version_dir = './generated-book-v1'\n",
    "if not os.path.exists(version_dir):\n",
    "    os.makedirs(version_dir)\n",
    "\n",
    "num_chapters = len([name for name in os.listdir(version_dir) if os.path.isfile(os.path.join(version_dir, name))])\n",
    "next_chapter = version_dir + '/chapter-' + str(num_chapters + 1) + '.md'\n",
    "with open(next_chapter, \"w\") as text_file:\n",
    "    text_file.write(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
