{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 407756\n"
     ]
    }
   ],
   "source": [
    "path = 'data_album/aesop_rock.txt'\n",
    "text = open(path).read().lower()\n",
    "\n",
    "print('Corpus length: {}'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 68\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print ('Total Characters: {}'.format(len(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_album/aesop_rock.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int_to_char = dict(enumerate(chars))\n",
    "char_to_int = {character : index for index, character in int_to_char.items()}\n",
    "encoded = np.array([char_to_int[character] for character in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # initialize the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype = np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    \n",
    "    # reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''\n",
    "    Create a generator that returns mini-batches\n",
    "    of size n_seqs x n_steps from arr\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches*batch_size]\n",
    "    # reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # the features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # the targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 tokens, \n",
    "                 n_steps = 100, \n",
    "                 n_hidden = 256, \n",
    "                 n_layers = 2, \n",
    "                 drop_prob = 0.5, \n",
    "                 lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int_to_char = dict(enumerate(self.chars))\n",
    "        self.char_to_int = {character:index for index, character in self.int_to_char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars),\n",
    "                           n_hidden,\n",
    "                           n_layers,\n",
    "                           dropout = drop_prob,\n",
    "                           batch_first = True)\n",
    "        self.fc = nn.Linear(n_hidden,\n",
    "                           len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self,\n",
    "               x,\n",
    "               hc):\n",
    "        '''\n",
    "        Forward pass through the network\n",
    "        '''\n",
    "        \n",
    "        x,(h,c) = self.lstm(x,hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # stack up LSTM outputs\n",
    "        x = x.view(x.size()[0] * x.size()[1],\n",
    "                  self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h,c)\n",
    "        \n",
    "    def predict(self,\n",
    "               char,\n",
    "               h = None,\n",
    "               cuda = False,\n",
    "               top_k = None):\n",
    "        '''\n",
    "        Given a character, predict the next character\n",
    "        Returns the predicted character and the hidden state\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char_to_int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x),\n",
    "                         volatile = True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile = True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "        \n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p = p/p.sum())\n",
    "        \n",
    "        return self.int_to_char[char], h\n",
    "        \n",
    "    \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initialize weights for fully connected layer\n",
    "        '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        \n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def init_hidden(self,\n",
    "                   n_seqs):\n",
    "        '''\n",
    "        Initialize hidden state\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net,\n",
    "         data,\n",
    "         epochs = 10,\n",
    "         n_seqs = 10,\n",
    "         n_steps = 50,\n",
    "         lr = 0.001,\n",
    "         clip = 5,\n",
    "         val_frac = 0.1,\n",
    "         cuda = False,\n",
    "         print_every = 10):\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_index = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_index], data[val_index:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_characters = len(net.chars)\n",
    "    for epoch in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_characters)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # Create new variables for the hidden state\n",
    "            # otherwise, we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # clip_grad_norm helps prevent the explodng gradient problem in RNNs\n",
    "            nn.utils.clip_grad_norm(net.parameters(),clip)\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    x = one_hot_encode(x, n_characters)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([Variable(each.data, volatile = True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile = True), Variable(y, volatile = True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                    \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                    \n",
    "                print('====================',\n",
    "                      'Epoch: {}/{} ...'.format(epoch+1, epochs),\n",
    "                      'Step: {}...'.format(counter),\n",
    "                      'Loss: {:.4f}...'.format(loss.data[0]),\n",
    "                      'Validation Loss: {:.4f}...'.format(np.mean(val_losses)),\n",
    "                      '====================') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars,\n",
    "             n_hidden = 512,\n",
    "             n_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch: 1/50 ... Step: 2... Loss: 4.1354... Validation Loss: 3.7808... ====================\n",
      "==================== Epoch: 1/50 ... Step: 4... Loss: 3.5732... Validation Loss: 3.5664... ====================\n",
      "==================== Epoch: 1/50 ... Step: 6... Loss: 3.4918... Validation Loss: 3.4549... ====================\n",
      "==================== Epoch: 1/50 ... Step: 8... Loss: 3.4462... Validation Loss: 3.4056... ====================\n",
      "==================== Epoch: 1/50 ... Step: 10... Loss: 3.4078... Validation Loss: 3.3947... ====================\n",
      "==================== Epoch: 1/50 ... Step: 12... Loss: 3.3791... Validation Loss: 3.3667... ====================\n",
      "==================== Epoch: 1/50 ... Step: 14... Loss: 3.3562... Validation Loss: 3.3421... ====================\n",
      "==================== Epoch: 1/50 ... Step: 16... Loss: 3.3036... Validation Loss: 3.3238... ====================\n",
      "==================== Epoch: 1/50 ... Step: 18... Loss: 3.3140... Validation Loss: 3.3098... ====================\n",
      "==================== Epoch: 1/50 ... Step: 20... Loss: 3.2750... Validation Loss: 3.2846... ====================\n",
      "==================== Epoch: 1/50 ... Step: 22... Loss: 3.2972... Validation Loss: 3.2623... ====================\n",
      "==================== Epoch: 1/50 ... Step: 24... Loss: 3.2469... Validation Loss: 3.2553... ====================\n",
      "==================== Epoch: 1/50 ... Step: 26... Loss: 3.2343... Validation Loss: 3.2281... ====================\n",
      "==================== Epoch: 1/50 ... Step: 28... Loss: 3.2340... Validation Loss: 3.1938... ====================\n",
      "==================== Epoch: 2/50 ... Step: 30... Loss: 3.2060... Validation Loss: 3.1854... ====================\n",
      "==================== Epoch: 2/50 ... Step: 32... Loss: 3.1717... Validation Loss: 3.1596... ====================\n",
      "==================== Epoch: 2/50 ... Step: 34... Loss: 3.1570... Validation Loss: 3.1290... ====================\n",
      "==================== Epoch: 2/50 ... Step: 36... Loss: 3.1166... Validation Loss: 3.1050... ====================\n",
      "==================== Epoch: 2/50 ... Step: 38... Loss: 3.1027... Validation Loss: 3.0788... ====================\n",
      "==================== Epoch: 2/50 ... Step: 40... Loss: 3.0706... Validation Loss: 3.0514... ====================\n",
      "==================== Epoch: 2/50 ... Step: 42... Loss: 3.0340... Validation Loss: 3.0253... ====================\n",
      "==================== Epoch: 2/50 ... Step: 44... Loss: 2.9824... Validation Loss: 2.9950... ====================\n",
      "==================== Epoch: 2/50 ... Step: 46... Loss: 2.9675... Validation Loss: 2.9708... ====================\n",
      "==================== Epoch: 2/50 ... Step: 48... Loss: 2.9199... Validation Loss: 2.9346... ====================\n",
      "==================== Epoch: 2/50 ... Step: 50... Loss: 2.9257... Validation Loss: 2.9094... ====================\n",
      "==================== Epoch: 2/50 ... Step: 52... Loss: 2.8706... Validation Loss: 2.8876... ====================\n",
      "==================== Epoch: 2/50 ... Step: 54... Loss: 2.8568... Validation Loss: 2.8511... ====================\n",
      "==================== Epoch: 2/50 ... Step: 56... Loss: 2.8726... Validation Loss: 2.8217... ====================\n",
      "==================== Epoch: 3/50 ... Step: 58... Loss: 2.8333... Validation Loss: 2.8087... ====================\n",
      "==================== Epoch: 3/50 ... Step: 60... Loss: 2.8067... Validation Loss: 2.7923... ====================\n",
      "==================== Epoch: 3/50 ... Step: 62... Loss: 2.7984... Validation Loss: 2.7727... ====================\n",
      "==================== Epoch: 3/50 ... Step: 64... Loss: 2.7587... Validation Loss: 2.7596... ====================\n",
      "==================== Epoch: 3/50 ... Step: 66... Loss: 2.7710... Validation Loss: 2.7448... ====================\n",
      "==================== Epoch: 3/50 ... Step: 68... Loss: 2.7326... Validation Loss: 2.7354... ====================\n",
      "==================== Epoch: 3/50 ... Step: 70... Loss: 2.7106... Validation Loss: 2.7177... ====================\n",
      "==================== Epoch: 3/50 ... Step: 72... Loss: 2.6821... Validation Loss: 2.6986... ====================\n",
      "==================== Epoch: 3/50 ... Step: 74... Loss: 2.6804... Validation Loss: 2.6936... ====================\n",
      "==================== Epoch: 3/50 ... Step: 76... Loss: 2.6386... Validation Loss: 2.6768... ====================\n",
      "==================== Epoch: 3/50 ... Step: 78... Loss: 2.6797... Validation Loss: 2.6695... ====================\n",
      "==================== Epoch: 3/50 ... Step: 80... Loss: 2.6420... Validation Loss: 2.6561... ====================\n",
      "==================== Epoch: 3/50 ... Step: 82... Loss: 2.6468... Validation Loss: 2.6520... ====================\n",
      "==================== Epoch: 3/50 ... Step: 84... Loss: 2.6707... Validation Loss: 2.6386... ====================\n",
      "==================== Epoch: 4/50 ... Step: 86... Loss: 2.6355... Validation Loss: 2.6258... ====================\n",
      "==================== Epoch: 4/50 ... Step: 88... Loss: 2.6202... Validation Loss: 2.6162... ====================\n",
      "==================== Epoch: 4/50 ... Step: 90... Loss: 2.6209... Validation Loss: 2.6096... ====================\n",
      "==================== Epoch: 4/50 ... Step: 92... Loss: 2.5963... Validation Loss: 2.5978... ====================\n",
      "==================== Epoch: 4/50 ... Step: 94... Loss: 2.6253... Validation Loss: 2.5867... ====================\n",
      "==================== Epoch: 4/50 ... Step: 96... Loss: 2.5920... Validation Loss: 2.5842... ====================\n",
      "==================== Epoch: 4/50 ... Step: 98... Loss: 2.5597... Validation Loss: 2.5739... ====================\n",
      "==================== Epoch: 4/50 ... Step: 100... Loss: 2.5347... Validation Loss: 2.5616... ====================\n",
      "==================== Epoch: 4/50 ... Step: 102... Loss: 2.5425... Validation Loss: 2.5600... ====================\n",
      "==================== Epoch: 4/50 ... Step: 104... Loss: 2.5078... Validation Loss: 2.5554... ====================\n",
      "==================== Epoch: 4/50 ... Step: 106... Loss: 2.5494... Validation Loss: 2.5513... ====================\n",
      "==================== Epoch: 4/50 ... Step: 108... Loss: 2.5205... Validation Loss: 2.5421... ====================\n",
      "==================== Epoch: 4/50 ... Step: 110... Loss: 2.5343... Validation Loss: 2.5315... ====================\n",
      "==================== Epoch: 4/50 ... Step: 112... Loss: 2.5632... Validation Loss: 2.5280... ====================\n",
      "==================== Epoch: 5/50 ... Step: 114... Loss: 2.5291... Validation Loss: 2.5185... ====================\n",
      "==================== Epoch: 5/50 ... Step: 116... Loss: 2.5133... Validation Loss: 2.5187... ====================\n",
      "==================== Epoch: 5/50 ... Step: 118... Loss: 2.5163... Validation Loss: 2.5083... ====================\n",
      "==================== Epoch: 5/50 ... Step: 120... Loss: 2.4946... Validation Loss: 2.5024... ====================\n",
      "==================== Epoch: 5/50 ... Step: 122... Loss: 2.5242... Validation Loss: 2.4927... ====================\n",
      "==================== Epoch: 5/50 ... Step: 124... Loss: 2.4884... Validation Loss: 2.4916... ====================\n",
      "==================== Epoch: 5/50 ... Step: 126... Loss: 2.4736... Validation Loss: 2.4904... ====================\n",
      "==================== Epoch: 5/50 ... Step: 128... Loss: 2.4388... Validation Loss: 2.4808... ====================\n",
      "==================== Epoch: 5/50 ... Step: 130... Loss: 2.4669... Validation Loss: 2.4838... ====================\n",
      "==================== Epoch: 5/50 ... Step: 132... Loss: 2.4322... Validation Loss: 2.4766... ====================\n",
      "==================== Epoch: 5/50 ... Step: 134... Loss: 2.4616... Validation Loss: 2.4754... ====================\n",
      "==================== Epoch: 5/50 ... Step: 136... Loss: 2.4508... Validation Loss: 2.4680... ====================\n",
      "==================== Epoch: 5/50 ... Step: 138... Loss: 2.4433... Validation Loss: 2.4620... ====================\n",
      "==================== Epoch: 5/50 ... Step: 140... Loss: 2.4863... Validation Loss: 2.4622... ====================\n",
      "==================== Epoch: 6/50 ... Step: 142... Loss: 2.4483... Validation Loss: 2.4590... ====================\n",
      "==================== Epoch: 6/50 ... Step: 144... Loss: 2.4463... Validation Loss: 2.4499... ====================\n",
      "==================== Epoch: 6/50 ... Step: 146... Loss: 2.4437... Validation Loss: 2.4475... ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch: 6/50 ... Step: 148... Loss: 2.4296... Validation Loss: 2.4447... ====================\n",
      "==================== Epoch: 6/50 ... Step: 150... Loss: 2.4597... Validation Loss: 2.4378... ====================\n",
      "==================== Epoch: 6/50 ... Step: 152... Loss: 2.4230... Validation Loss: 2.4351... ====================\n",
      "==================== Epoch: 6/50 ... Step: 154... Loss: 2.3967... Validation Loss: 2.4326... ====================\n",
      "==================== Epoch: 6/50 ... Step: 156... Loss: 2.3932... Validation Loss: 2.4279... ====================\n",
      "==================== Epoch: 6/50 ... Step: 158... Loss: 2.3971... Validation Loss: 2.4237... ====================\n",
      "==================== Epoch: 6/50 ... Step: 160... Loss: 2.3686... Validation Loss: 2.4239... ====================\n",
      "==================== Epoch: 6/50 ... Step: 162... Loss: 2.4064... Validation Loss: 2.4145... ====================\n",
      "==================== Epoch: 6/50 ... Step: 164... Loss: 2.3928... Validation Loss: 2.4190... ====================\n",
      "==================== Epoch: 6/50 ... Step: 166... Loss: 2.3911... Validation Loss: 2.4145... ====================\n",
      "==================== Epoch: 6/50 ... Step: 168... Loss: 2.4411... Validation Loss: 2.4138... ====================\n",
      "==================== Epoch: 7/50 ... Step: 170... Loss: 2.3911... Validation Loss: 2.4088... ====================\n",
      "==================== Epoch: 7/50 ... Step: 172... Loss: 2.3953... Validation Loss: 2.4037... ====================\n",
      "==================== Epoch: 7/50 ... Step: 174... Loss: 2.3994... Validation Loss: 2.3971... ====================\n",
      "==================== Epoch: 7/50 ... Step: 176... Loss: 2.3902... Validation Loss: 2.3970... ====================\n",
      "==================== Epoch: 7/50 ... Step: 178... Loss: 2.3953... Validation Loss: 2.3888... ====================\n",
      "==================== Epoch: 7/50 ... Step: 180... Loss: 2.3739... Validation Loss: 2.3932... ====================\n",
      "==================== Epoch: 7/50 ... Step: 182... Loss: 2.3565... Validation Loss: 2.3824... ====================\n",
      "==================== Epoch: 7/50 ... Step: 184... Loss: 2.3330... Validation Loss: 2.3810... ====================\n",
      "==================== Epoch: 7/50 ... Step: 186... Loss: 2.3531... Validation Loss: 2.3808... ====================\n",
      "==================== Epoch: 7/50 ... Step: 188... Loss: 2.3161... Validation Loss: 2.3805... ====================\n",
      "==================== Epoch: 7/50 ... Step: 190... Loss: 2.3567... Validation Loss: 2.3749... ====================\n",
      "==================== Epoch: 7/50 ... Step: 192... Loss: 2.3523... Validation Loss: 2.3708... ====================\n",
      "==================== Epoch: 7/50 ... Step: 194... Loss: 2.3392... Validation Loss: 2.3727... ====================\n",
      "==================== Epoch: 7/50 ... Step: 196... Loss: 2.3868... Validation Loss: 2.3660... ====================\n",
      "==================== Epoch: 8/50 ... Step: 198... Loss: 2.3490... Validation Loss: 2.3689... ====================\n",
      "==================== Epoch: 8/50 ... Step: 200... Loss: 2.3460... Validation Loss: 2.3616... ====================\n",
      "==================== Epoch: 8/50 ... Step: 202... Loss: 2.3522... Validation Loss: 2.3540... ====================\n",
      "==================== Epoch: 8/50 ... Step: 204... Loss: 2.3358... Validation Loss: 2.3472... ====================\n",
      "==================== Epoch: 8/50 ... Step: 206... Loss: 2.3549... Validation Loss: 2.3492... ====================\n",
      "==================== Epoch: 8/50 ... Step: 208... Loss: 2.3250... Validation Loss: 2.3473... ====================\n",
      "==================== Epoch: 8/50 ... Step: 210... Loss: 2.3153... Validation Loss: 2.3402... ====================\n",
      "==================== Epoch: 8/50 ... Step: 212... Loss: 2.2939... Validation Loss: 2.3457... ====================\n",
      "==================== Epoch: 8/50 ... Step: 214... Loss: 2.2950... Validation Loss: 2.3489... ====================\n",
      "==================== Epoch: 8/50 ... Step: 216... Loss: 2.2730... Validation Loss: 2.3415... ====================\n",
      "==================== Epoch: 8/50 ... Step: 218... Loss: 2.3088... Validation Loss: 2.3380... ====================\n",
      "==================== Epoch: 8/50 ... Step: 220... Loss: 2.3115... Validation Loss: 2.3304... ====================\n",
      "==================== Epoch: 8/50 ... Step: 222... Loss: 2.2934... Validation Loss: 2.3378... ====================\n",
      "==================== Epoch: 8/50 ... Step: 224... Loss: 2.3379... Validation Loss: 2.3346... ====================\n",
      "==================== Epoch: 9/50 ... Step: 226... Loss: 2.3117... Validation Loss: 2.3324... ====================\n",
      "==================== Epoch: 9/50 ... Step: 228... Loss: 2.3009... Validation Loss: 2.3258... ====================\n",
      "==================== Epoch: 9/50 ... Step: 230... Loss: 2.3041... Validation Loss: 2.3306... ====================\n",
      "==================== Epoch: 9/50 ... Step: 232... Loss: 2.3107... Validation Loss: 2.3173... ====================\n",
      "==================== Epoch: 9/50 ... Step: 234... Loss: 2.3179... Validation Loss: 2.3206... ====================\n",
      "==================== Epoch: 9/50 ... Step: 236... Loss: 2.2866... Validation Loss: 2.3151... ====================\n",
      "==================== Epoch: 9/50 ... Step: 238... Loss: 2.2744... Validation Loss: 2.3121... ====================\n",
      "==================== Epoch: 9/50 ... Step: 240... Loss: 2.2456... Validation Loss: 2.3073... ====================\n",
      "==================== Epoch: 9/50 ... Step: 242... Loss: 2.2679... Validation Loss: 2.3136... ====================\n",
      "==================== Epoch: 9/50 ... Step: 244... Loss: 2.2412... Validation Loss: 2.3044... ====================\n",
      "==================== Epoch: 9/50 ... Step: 246... Loss: 2.2719... Validation Loss: 2.2997... ====================\n",
      "==================== Epoch: 9/50 ... Step: 248... Loss: 2.2688... Validation Loss: 2.3061... ====================\n",
      "==================== Epoch: 9/50 ... Step: 250... Loss: 2.2569... Validation Loss: 2.2969... ====================\n",
      "==================== Epoch: 9/50 ... Step: 252... Loss: 2.3122... Validation Loss: 2.2993... ====================\n",
      "==================== Epoch: 10/50 ... Step: 254... Loss: 2.2813... Validation Loss: 2.3024... ====================\n",
      "==================== Epoch: 10/50 ... Step: 256... Loss: 2.2660... Validation Loss: 2.2902... ====================\n",
      "==================== Epoch: 10/50 ... Step: 258... Loss: 2.2864... Validation Loss: 2.2898... ====================\n",
      "==================== Epoch: 10/50 ... Step: 260... Loss: 2.2714... Validation Loss: 2.2891... ====================\n",
      "==================== Epoch: 10/50 ... Step: 262... Loss: 2.2813... Validation Loss: 2.2824... ====================\n",
      "==================== Epoch: 10/50 ... Step: 264... Loss: 2.2490... Validation Loss: 2.2891... ====================\n",
      "==================== Epoch: 10/50 ... Step: 266... Loss: 2.2299... Validation Loss: 2.2860... ====================\n",
      "==================== Epoch: 10/50 ... Step: 268... Loss: 2.2188... Validation Loss: 2.2800... ====================\n",
      "==================== Epoch: 10/50 ... Step: 270... Loss: 2.2381... Validation Loss: 2.2796... ====================\n",
      "==================== Epoch: 10/50 ... Step: 272... Loss: 2.2081... Validation Loss: 2.2798... ====================\n",
      "==================== Epoch: 10/50 ... Step: 274... Loss: 2.2552... Validation Loss: 2.2741... ====================\n",
      "==================== Epoch: 10/50 ... Step: 276... Loss: 2.2363... Validation Loss: 2.2811... ====================\n",
      "==================== Epoch: 10/50 ... Step: 278... Loss: 2.2234... Validation Loss: 2.2758... ====================\n",
      "==================== Epoch: 10/50 ... Step: 280... Loss: 2.2789... Validation Loss: 2.2704... ====================\n",
      "==================== Epoch: 11/50 ... Step: 282... Loss: 2.2444... Validation Loss: 2.2683... ====================\n",
      "==================== Epoch: 11/50 ... Step: 284... Loss: 2.2312... Validation Loss: 2.2652... ====================\n",
      "==================== Epoch: 11/50 ... Step: 286... Loss: 2.2476... Validation Loss: 2.2677... ====================\n",
      "==================== Epoch: 11/50 ... Step: 288... Loss: 2.2478... Validation Loss: 2.2651... ====================\n",
      "==================== Epoch: 11/50 ... Step: 290... Loss: 2.2550... Validation Loss: 2.2659... ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch: 11/50 ... Step: 292... Loss: 2.2206... Validation Loss: 2.2604... ====================\n",
      "==================== Epoch: 11/50 ... Step: 294... Loss: 2.2069... Validation Loss: 2.2629... ====================\n",
      "==================== Epoch: 11/50 ... Step: 296... Loss: 2.1863... Validation Loss: 2.2545... ====================\n",
      "==================== Epoch: 11/50 ... Step: 298... Loss: 2.2069... Validation Loss: 2.2558... ====================\n",
      "==================== Epoch: 11/50 ... Step: 300... Loss: 2.1895... Validation Loss: 2.2677... ====================\n",
      "==================== Epoch: 11/50 ... Step: 302... Loss: 2.2225... Validation Loss: 2.2516... ====================\n",
      "==================== Epoch: 11/50 ... Step: 304... Loss: 2.2162... Validation Loss: 2.2576... ====================\n",
      "==================== Epoch: 11/50 ... Step: 306... Loss: 2.2000... Validation Loss: 2.2524... ====================\n",
      "==================== Epoch: 11/50 ... Step: 308... Loss: 2.2548... Validation Loss: 2.2497... ====================\n",
      "==================== Epoch: 12/50 ... Step: 310... Loss: 2.2134... Validation Loss: 2.2464... ====================\n",
      "==================== Epoch: 12/50 ... Step: 312... Loss: 2.2157... Validation Loss: 2.2506... ====================\n",
      "==================== Epoch: 12/50 ... Step: 314... Loss: 2.2235... Validation Loss: 2.2447... ====================\n",
      "==================== Epoch: 12/50 ... Step: 316... Loss: 2.2084... Validation Loss: 2.2443... ====================\n",
      "==================== Epoch: 12/50 ... Step: 318... Loss: 2.2252... Validation Loss: 2.2386... ====================\n",
      "==================== Epoch: 12/50 ... Step: 320... Loss: 2.2023... Validation Loss: 2.2401... ====================\n",
      "==================== Epoch: 12/50 ... Step: 322... Loss: 2.1715... Validation Loss: 2.2386... ====================\n",
      "==================== Epoch: 12/50 ... Step: 324... Loss: 2.1516... Validation Loss: 2.2410... ====================\n",
      "==================== Epoch: 12/50 ... Step: 326... Loss: 2.1722... Validation Loss: 2.2341... ====================\n",
      "==================== Epoch: 12/50 ... Step: 328... Loss: 2.1588... Validation Loss: 2.2328... ====================\n",
      "==================== Epoch: 12/50 ... Step: 330... Loss: 2.1914... Validation Loss: 2.2318... ====================\n",
      "==================== Epoch: 12/50 ... Step: 332... Loss: 2.1805... Validation Loss: 2.2327... ====================\n",
      "==================== Epoch: 12/50 ... Step: 334... Loss: 2.1698... Validation Loss: 2.2310... ====================\n",
      "==================== Epoch: 12/50 ... Step: 336... Loss: 2.2265... Validation Loss: 2.2247... ====================\n",
      "==================== Epoch: 13/50 ... Step: 338... Loss: 2.1905... Validation Loss: 2.2272... ====================\n",
      "==================== Epoch: 13/50 ... Step: 340... Loss: 2.1818... Validation Loss: 2.2246... ====================\n",
      "==================== Epoch: 13/50 ... Step: 342... Loss: 2.1943... Validation Loss: 2.2236... ====================\n",
      "==================== Epoch: 13/50 ... Step: 344... Loss: 2.1804... Validation Loss: 2.2178... ====================\n",
      "==================== Epoch: 13/50 ... Step: 346... Loss: 2.1953... Validation Loss: 2.2188... ====================\n",
      "==================== Epoch: 13/50 ... Step: 348... Loss: 2.1666... Validation Loss: 2.2158... ====================\n",
      "==================== Epoch: 13/50 ... Step: 350... Loss: 2.1502... Validation Loss: 2.2132... ====================\n",
      "==================== Epoch: 13/50 ... Step: 352... Loss: 2.1307... Validation Loss: 2.2197... ====================\n",
      "==================== Epoch: 13/50 ... Step: 354... Loss: 2.1653... Validation Loss: 2.2186... ====================\n",
      "==================== Epoch: 13/50 ... Step: 356... Loss: 2.1379... Validation Loss: 2.2120... ====================\n",
      "==================== Epoch: 13/50 ... Step: 358... Loss: 2.1643... Validation Loss: 2.2093... ====================\n",
      "==================== Epoch: 13/50 ... Step: 360... Loss: 2.1645... Validation Loss: 2.2112... ====================\n",
      "==================== Epoch: 13/50 ... Step: 362... Loss: 2.1514... Validation Loss: 2.2119... ====================\n",
      "==================== Epoch: 13/50 ... Step: 364... Loss: 2.2022... Validation Loss: 2.2021... ====================\n",
      "==================== Epoch: 14/50 ... Step: 366... Loss: 2.1652... Validation Loss: 2.2067... ====================\n",
      "==================== Epoch: 14/50 ... Step: 368... Loss: 2.1629... Validation Loss: 2.2027... ====================\n",
      "==================== Epoch: 14/50 ... Step: 370... Loss: 2.1737... Validation Loss: 2.2019... ====================\n",
      "==================== Epoch: 14/50 ... Step: 372... Loss: 2.1751... Validation Loss: 2.1966... ====================\n",
      "==================== Epoch: 14/50 ... Step: 374... Loss: 2.1841... Validation Loss: 2.1974... ====================\n",
      "==================== Epoch: 14/50 ... Step: 376... Loss: 2.1482... Validation Loss: 2.2017... ====================\n",
      "==================== Epoch: 14/50 ... Step: 378... Loss: 2.1352... Validation Loss: 2.2010... ====================\n",
      "==================== Epoch: 14/50 ... Step: 380... Loss: 2.1048... Validation Loss: 2.1930... ====================\n",
      "==================== Epoch: 14/50 ... Step: 382... Loss: 2.1372... Validation Loss: 2.2034... ====================\n",
      "==================== Epoch: 14/50 ... Step: 384... Loss: 2.1141... Validation Loss: 2.1943... ====================\n",
      "==================== Epoch: 14/50 ... Step: 386... Loss: 2.1415... Validation Loss: 2.1951... ====================\n",
      "==================== Epoch: 14/50 ... Step: 388... Loss: 2.1318... Validation Loss: 2.1973... ====================\n",
      "==================== Epoch: 14/50 ... Step: 390... Loss: 2.1273... Validation Loss: 2.1884... ====================\n",
      "==================== Epoch: 14/50 ... Step: 392... Loss: 2.1736... Validation Loss: 2.1893... ====================\n",
      "==================== Epoch: 15/50 ... Step: 394... Loss: 2.1517... Validation Loss: 2.1936... ====================\n",
      "==================== Epoch: 15/50 ... Step: 396... Loss: 2.1453... Validation Loss: 2.1937... ====================\n",
      "==================== Epoch: 15/50 ... Step: 398... Loss: 2.1402... Validation Loss: 2.1881... ====================\n",
      "==================== Epoch: 15/50 ... Step: 400... Loss: 2.1390... Validation Loss: 2.1883... ====================\n",
      "==================== Epoch: 15/50 ... Step: 402... Loss: 2.1581... Validation Loss: 2.1831... ====================\n",
      "==================== Epoch: 15/50 ... Step: 404... Loss: 2.1237... Validation Loss: 2.1803... ====================\n",
      "==================== Epoch: 15/50 ... Step: 406... Loss: 2.1100... Validation Loss: 2.1773... ====================\n",
      "==================== Epoch: 15/50 ... Step: 408... Loss: 2.0886... Validation Loss: 2.1851... ====================\n",
      "==================== Epoch: 15/50 ... Step: 410... Loss: 2.1035... Validation Loss: 2.1765... ====================\n",
      "==================== Epoch: 15/50 ... Step: 412... Loss: 2.0887... Validation Loss: 2.1755... ====================\n",
      "==================== Epoch: 15/50 ... Step: 414... Loss: 2.1118... Validation Loss: 2.1723... ====================\n",
      "==================== Epoch: 15/50 ... Step: 416... Loss: 2.1192... Validation Loss: 2.1742... ====================\n",
      "==================== Epoch: 15/50 ... Step: 418... Loss: 2.0993... Validation Loss: 2.1711... ====================\n",
      "==================== Epoch: 15/50 ... Step: 420... Loss: 2.1487... Validation Loss: 2.1705... ====================\n",
      "==================== Epoch: 16/50 ... Step: 422... Loss: 2.1294... Validation Loss: 2.1727... ====================\n",
      "==================== Epoch: 16/50 ... Step: 424... Loss: 2.1137... Validation Loss: 2.1749... ====================\n",
      "==================== Epoch: 16/50 ... Step: 426... Loss: 2.1234... Validation Loss: 2.1745... ====================\n",
      "==================== Epoch: 16/50 ... Step: 428... Loss: 2.1132... Validation Loss: 2.1625... ====================\n",
      "==================== Epoch: 16/50 ... Step: 430... Loss: 2.1243... Validation Loss: 2.1635... ====================\n",
      "==================== Epoch: 16/50 ... Step: 432... Loss: 2.1036... Validation Loss: 2.1659... ====================\n",
      "==================== Epoch: 16/50 ... Step: 434... Loss: 2.0778... Validation Loss: 2.1616... ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch: 16/50 ... Step: 436... Loss: 2.0656... Validation Loss: 2.1682... ====================\n",
      "==================== Epoch: 16/50 ... Step: 438... Loss: 2.0864... Validation Loss: 2.1621... ====================\n",
      "==================== Epoch: 16/50 ... Step: 440... Loss: 2.0709... Validation Loss: 2.1639... ====================\n",
      "==================== Epoch: 16/50 ... Step: 442... Loss: 2.0929... Validation Loss: 2.1615... ====================\n",
      "==================== Epoch: 16/50 ... Step: 444... Loss: 2.0906... Validation Loss: 2.1617... ====================\n",
      "==================== Epoch: 16/50 ... Step: 446... Loss: 2.0824... Validation Loss: 2.1541... ====================\n",
      "==================== Epoch: 16/50 ... Step: 448... Loss: 2.1356... Validation Loss: 2.1631... ====================\n",
      "==================== Epoch: 17/50 ... Step: 450... Loss: 2.1174... Validation Loss: 2.1581... ====================\n",
      "==================== Epoch: 17/50 ... Step: 452... Loss: 2.1022... Validation Loss: 2.1515... ====================\n",
      "==================== Epoch: 17/50 ... Step: 454... Loss: 2.1116... Validation Loss: 2.1569... ====================\n",
      "==================== Epoch: 17/50 ... Step: 456... Loss: 2.1002... Validation Loss: 2.1538... ====================\n",
      "==================== Epoch: 17/50 ... Step: 458... Loss: 2.1018... Validation Loss: 2.1540... ====================\n",
      "==================== Epoch: 17/50 ... Step: 460... Loss: 2.0779... Validation Loss: 2.1529... ====================\n",
      "==================== Epoch: 17/50 ... Step: 462... Loss: 2.0672... Validation Loss: 2.1493... ====================\n",
      "==================== Epoch: 17/50 ... Step: 464... Loss: 2.0491... Validation Loss: 2.1490... ====================\n",
      "==================== Epoch: 17/50 ... Step: 466... Loss: 2.0685... Validation Loss: 2.1527... ====================\n",
      "==================== Epoch: 17/50 ... Step: 468... Loss: 2.0575... Validation Loss: 2.1528... ====================\n",
      "==================== Epoch: 17/50 ... Step: 470... Loss: 2.0838... Validation Loss: 2.1462... ====================\n",
      "==================== Epoch: 17/50 ... Step: 472... Loss: 2.0657... Validation Loss: 2.1481... ====================\n",
      "==================== Epoch: 17/50 ... Step: 474... Loss: 2.0600... Validation Loss: 2.1444... ====================\n",
      "==================== Epoch: 17/50 ... Step: 476... Loss: 2.1127... Validation Loss: 2.1484... ====================\n",
      "==================== Epoch: 18/50 ... Step: 478... Loss: 2.0895... Validation Loss: 2.1436... ====================\n",
      "==================== Epoch: 18/50 ... Step: 480... Loss: 2.0749... Validation Loss: 2.1368... ====================\n",
      "==================== Epoch: 18/50 ... Step: 482... Loss: 2.0800... Validation Loss: 2.1475... ====================\n",
      "==================== Epoch: 18/50 ... Step: 484... Loss: 2.0811... Validation Loss: 2.1364... ====================\n",
      "==================== Epoch: 18/50 ... Step: 486... Loss: 2.0923... Validation Loss: 2.1364... ====================\n",
      "==================== Epoch: 18/50 ... Step: 488... Loss: 2.0583... Validation Loss: 2.1360... ====================\n",
      "==================== Epoch: 18/50 ... Step: 490... Loss: 2.0414... Validation Loss: 2.1380... ====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-72feed94c2aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m      \u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# CUDA support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m      print_every = print_every_2)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-2312aca61d83>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, n_seqs, n_steps, lr, clip, val_frac, cuda, print_every)\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-222d6dd2f1cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hc)\u001b[0m\n\u001b[1;32m     35\u001b[0m         '''\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mingate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mforgetgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforgetgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcellgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcellgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_autograd_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/_functions/pointwise.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, inplace)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "n_seqs = 128\n",
    "n_steps = 100\n",
    "lr = 0.001\n",
    "print_every_2 = 2\n",
    "\n",
    "train(net,\n",
    "     encoded,\n",
    "     epochs,\n",
    "     n_seqs,\n",
    "     n_steps,\n",
    "     lr,\n",
    "     cuda = False, # CUDA support\n",
    "     print_every = print_every_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net,\n",
    "          size,\n",
    "          prime = 'The',\n",
    "          top_k = None,\n",
    "          cuda = False):\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    chars = [character for character in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for character in prime:\n",
    "        char, h = net.predict(character,\n",
    "                              h,\n",
    "                              cuda = cuda,\n",
    "                              top_k = top_k)\n",
    "    \n",
    "    chars.append(char)\n",
    "    \n",
    "    for index in range(size):\n",
    "        char, h = net.predict(chars[-1],\n",
    "                              h,\n",
    "                              cuda = cuda,\n",
    "                              top_k = top_k)\n",
    "        chars.append(char)\n",
    "    \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aesop\n",
      "That's be arrugul sparting tages a surrain battommers and but the mers on the cortes is only of brocks this battering ship it\n",
      "Ass I see the little outs of dince\n",
      "Sen the dream draging dreams of the mart on that through the pigh and bring strubble\n",
      "And the pilling themest carrotor traised\n",
      "The sittomerid that trangion and sticks, better my spirted sected, and the serent call pose\n",
      "I save they are to adlight a sticks of my still tradge to bull of the pore and with the city faciou to make these me in a blum or the stareth\n",
      "We the bash of the pins it asson the coust and secred\n",
      "I crung allow the strough themild\n",
      "That's the simple offariss\n",
      "I ame cimalint first toother pactic ant merting\n",
      "And a thisked to ship a brake pigate\n",
      "I spon that along mill that climp ant market to the match\n",
      "I see the pley out of the black frem\n",
      "So I can a bod babage breedingershel out a pire asking the burlies and basiding\n",
      "She tones, the clossed to turn the break of burn in the break\n",
      "I'm a canny, I spread on the day the day turner on a sumbor\n",
      "Sectripping the crooks the pressed\n",
      "To calle spet a crook britche bring and cormities\n",
      "Ats when the peasent andiled the mangam brewning the space\n",
      "And I will spit it in my salder springs\n",
      "All of ships four tolled tall the shilly out the selent\n",
      "Somithing is solding as annellow shooking a than\n",
      "I wake of a cartang and all and still antections and stalloted cold sitch\n",
      "And where's han atching spotive somestard to manner the stirn and was to tell at a still then\n",
      "Blaim but on a bod of and a back and back\n",
      "Badered a strong and she the proturaty prumorace\n",
      "Assanged the charnest ald tricking times\n",
      "I see my seems for the spire toward with a baster pince moneters of the circling states befice they seens a coldrand\n",
      "I don't ragged to the mash to the carcon back\n",
      "Burny throw the stingies on the parcies it\n",
      "Stere the seaker soles to storm the parete stop onch a pricate and the promotes\n",
      "And all they way a poore to could the plead to muld my can to see my settler prome catter\n",
      "Make motherfucker bead to the stack\n",
      "Traged in a parn that cread a still of the plants and the bull\n",
      "The bendally porded of the plumper take man a to the sarvers and a pold mase the last the spine\n",
      "To been the dort deverseen pissing of the monstered\n",
      "Talking of the plan sompassion of the capatingle\n",
      "I strangamour the cruck sture the pornot all\n",
      "Shaveling my the mothir back alone time\n",
      "I clock the bencom will back to sunk in a place than better mere the place and the spers\n",
      "Whine improw timis thas senders to the span the both marnsholded\n",
      "And the chame and stel metal to before the bangs to the break or the with a bing and both bact of sended insode that stalling\n",
      "I'll be a part wings with miss im was a see as the past to a sore the buckle to be the muddle\n",
      "And I wanta kitchen that sit spended all they was and botter forsal canst can to mance the summer they said\n",
      "And I'm the burker the partats trace arting and cations then the sticker bottle\n",
      "What a let a pill bast the bead that deam them tround to chind\n",
      "I sat was the sat high toward to be something the prettice lifes off the munty mare to base the lumb and was the mile this menitifes\n",
      "The sterman telinations spast as one to crose and the misters\n",
      "To compose the stall and to my fienty to concres intoment turn\n",
      "That consicent an in the marter than allenticen\n",
      "But the mending seam a don't can of the ching and sugher beations the made\n",
      "And she deed of the commetis sters the prust trail and chunce the send and spitted to the pracem\n",
      "So wallie the money\n",
      "But the retic atalice in the parks which\n",
      "And I'm sunger toll off the batter back to see shicked it\n",
      "Someday set sacard fram burning out the mooden on the park to arm the poldars\n",
      "Saride a pack\n",
      "Brick be the wallest cormarinal tellettors of the sunclades\n",
      "To heart hand aronget soul the shand\n",
      "All I the warn throw while the rung onter patterics\n",
      "I strill a somether peace, shan staped the bot spared it brinks and tup it\n",
      "In’t this my chear a think to mad a barn on ole\n",
      "Asom a platies ast coldicter pilling alting\n",
      "Blew the but stellashed seatertalls that the frignterters\n",
      "String the cold at hand when the metiles and be the master trying the prease, I'm threadly water bulls in the pullate\n",
      "Tunter to through the fingers\n",
      "I'm hell a tallook simple or menith\n",
      "I crook in your pash and spil the pilling\n",
      "Boles and starvets triped in the face to beat the muddle\n",
      "I'm a seepin a can and low to bean the sarn and soldowing the self tenched of menstarts and to the cammit\n",
      "I'm a troubon trucker turned, so to breaked, shoulden and but they dreams on a blind off the brookent star\n",
      "Assal prodical well the mill burn\n",
      "I am a black case a tone on the pigs to the piggen britch\n",
      "Bicker a trunk and barn betoraming my cheaning\n",
      "And ship when a tence of a pilot polor to splock their fire\n",
      "The walk it of the but shit the bell breed\n",
      "I'm a searle see a clash the back wants to bring it world peal the paser starts bread in the manter\n",
      "I deal I see alk spother stard atanttom\n",
      "It's the same with and the pits and a this specially\n",
      "Stordy and the pr\n"
     ]
    }
   ],
   "source": [
    "print(sample(net,\n",
    "             5000,\n",
    "             prime = 'Aesop',\n",
    "             top_k=5,\n",
    "             cuda = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York those back and a stan and spreading outsed my servess tellors and sticks and steres\n",
      "To beline the callioned speting\n",
      "It's left she to damake to hand his as out a marty tricks\n",
      "To my starting the pire of all spirit stalling that spete the muttle from but it's settle me wentert\n",
      "Till it blind the bleaded teets bell the potal to masters\n",
      "And the pation of the pathages\n",
      "We themearn's brooder spensing fuck the shook and well\n",
      "Aed a the blood on a shin off internives\n",
      "Attic alove the sprung common the batch in\n",
      "Aed the mother fung it's the beak for the motime\n",
      "Ster in the stop\n",
      "Anto they drug of the buck on a crom of the clunch\n",
      "Bathon a shank and traped bad a mine and spon a stink bottle pasting ousto the morting the produmes\n",
      "It's let the prin a purnen achuse my channies\n",
      "That's the see a stolar some and turn to pull it\n",
      "Low that hung a beak and head the bast\n",
      "And I'm an the beff stop\n",
      "And I'm a fing that walked then compoter\n",
      "And a still me a so day, and dank, the cat apop the same\n",
      "To the can to benew themered the contringed\n",
      "I wean I slipped a second incruded\n",
      "The midele spill that spenes and halk of the perstical\n",
      "Where your calling mind if sull shook out my life to be the mare to stepper that prace\n",
      "Thousand and he cromp through the mores\n",
      "At a stroke off with a ship drink to the bott messience all a polating as a proconame with a mett clettack then petalested, asser that\n",
      "Same shoved one the prodict\n",
      "I'm on a brick around stittle flaming assics\n",
      "Shark the she dish dignd my pingen pattening servilation seeps the bormach then crew or pouns\n",
      "When the cherich prodotics linerally coust\n",
      "Well anter the plessions whe presture chascar trade and stard to and me with monstor to the most born soulsten patered clean and she preader\n",
      "And I crail the spin a starked off the but betomene shill pig the bitter past the porment to see who ponsions\n",
      "I the strettors, arrow the chere to the belled of the past\n",
      "Somea to the canta drunken dectic tell to blessed if the clomp if to spark the blocks turn to that spearing\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, cuda=False, top_k=5, prime=\"New York\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
